{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Armed Bandit Problem\n",
    "\n",
    "We will show multiple methods to solve this partial-information sequential decision making problem, referred to as the Multi Armed Bandit Problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "#np.set_printoptions(2)\n",
    "np.set_printoptions(3,suppress=True)\n",
    "np.random.seed(67)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon-greedy Agent method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_prob = np.random.rand(10)\n",
    "number_of_levers_count = np.zeros(10)\n",
    "\n",
    "agents_prob = np.zeros(10)\n",
    "reward_count = np.zeros(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episode = 4000\n",
    "e = 0.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(num_episode):\n",
    "    \n",
    "    # Either choose a greedy action or explore\n",
    "    if e > np.random.uniform():\n",
    "        which_lever_to_pull = np.random.randint(0,10)\n",
    "    else:\n",
    "        which_lever_to_pull = np.argmax(agents_prob)\n",
    "\n",
    "    # now pull the lever \n",
    "    if ground_truth_prob[which_lever_to_pull] > np.random.uniform():\n",
    "        reward = 1\n",
    "    else:\n",
    "        reward = 0\n",
    "\n",
    "    # now update the lever count and expected value\n",
    "    number_of_levers_count[which_lever_to_pull] += 1\n",
    "    reward_count[which_lever_to_pull] = reward_count[which_lever_to_pull] \\\n",
    "                                        + reward\n",
    "    agents_prob[which_lever_to_pull] = agents_prob[which_lever_to_pull] \\\n",
    "                    + (1/number_of_levers_count[which_lever_to_pull]) * \\\n",
    "                      (reward - agents_prob[which_lever_to_pull])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------\n",
      "Number of Levers Count Each:  [ 136. 2847.  121.  106.  118.  120.  149.  135.  123.  145.]\n",
      "Sum of lever must match with # episode :  4000.0  :  4000\n"
     ]
    }
   ],
   "source": [
    "print('\\n-------------------------')\n",
    "print(\"Number of Levers Count Each: \", number_of_levers_count)\n",
    "print(\"Sum of lever must match with # episode : \",\n",
    "      number_of_levers_count.sum(), ' : ', num_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------\n",
      "Reward Over Time Tracking :  [ 111. 2803.   34.   38.   34.  101.   11.  113.   38.   42.]\n",
      "Agent's weight :  [0.82 0.98 0.28 0.36 0.29 0.84 0.07 0.84 0.31 0.29]\n",
      "Agent's Probability Guess :  [0.82 0.98 0.28 0.36 0.29 0.84 0.07 0.84 0.31 0.29]\n",
      "Agents Guess Best Lever :  1\n"
     ]
    }
   ],
   "source": [
    "print('\\n-------------------------')\n",
    "print(\"Reward Over Time Tracking : \", reward_count)\n",
    "print(\"Agent's weight : \", agents_prob)\n",
    "print(\"Agent's Probability Guess : \", reward_count/number_of_levers_count )\n",
    "print(\"Agents Guess Best Lever : \", \n",
    "      np.argmax(reward_count/number_of_levers_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------\n",
      "Ground Truth Full list :  [0.81 0.99 0.29 0.39 0.27 0.83 0.1  0.86 0.34 0.36]\n",
      "Ground Truth Best Lever :  1\n"
     ]
    }
   ],
   "source": [
    "print('\\n-------------------------')\n",
    "print(\"Ground Truth Full list : \", ground_truth_prob)\n",
    "print(\"Ground Truth Best Lever : \", np.argmax(ground_truth_prob))\n",
    "\n",
    "# -- end code --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Gradient with log-likelihood \n",
    "\n",
    "We now are going to update the agent’s weight. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_prob = np.random.rand(10)\n",
    "number_of_levers_count = np.zeros(10)\n",
    "agents_prob  = np.zeros(10)\n",
    "reward_count = np.zeros(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episode = 10000\n",
    "e = 0.99\n",
    "learning_rate = 0.5   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(num_episode):\n",
    "    \n",
    "    # Either choose a greedy action or explore\n",
    "    if e > np.random.uniform():\n",
    "        which_lever_to_pull = np.random.randint(0,10)\n",
    "    else:\n",
    "        which_lever_to_pull = np.argmax(agents_prob)\n",
    "\n",
    "    # now pull the lever \n",
    "    if ground_truth_prob[which_lever_to_pull] > np.random.uniform():\n",
    "        reward = 1\n",
    "    else:\n",
    "        reward = 0\n",
    "\n",
    "    # now update the lever count and expected value\n",
    "    number_of_levers_count[which_lever_to_pull] += 1\n",
    "    loss =  - np.log( agents_prob[which_lever_to_pull] + 1e-3 ) * reward \n",
    "    agents_prob[which_lever_to_pull] = agents_prob[which_lever_to_pull] \\\n",
    "    - learning_rate * (-1/(agents_prob[which_lever_to_pull]+1e-3)) * \\\n",
    "      reward\n",
    "    reward_count[which_lever_to_pull] = reward_count[which_lever_to_pull] \\\n",
    "                                        + reward\n",
    "    e = e * 0.9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------\n",
      "Number of Levers Count Each:  [ 653.  635.  618.  633.  647.  591.  650. 4363.  615.  595.]\n",
      "Sum of lever must match with # episode :  10000.0  :  10000\n"
     ]
    }
   ],
   "source": [
    "print('\\n-------------------------')\n",
    "print(\"Number of Levers Count Each: \", number_of_levers_count)\n",
    "print(\"Sum of lever must match with # episode : \",number_of_levers_count.sum(), ' : ',\n",
    "      num_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------\n",
      "Reward Over Time Tracking :  [ 361.  534.  449.  215.   46.  223.  133. 4060.  451.   24.]\n",
      "Agent's weight :  [500.36  500.533 500.448 500.214 500.045 500.222 500.132 504.043 500.45\n",
      " 500.023]\n",
      "Agent's Probability Guess :  [0.553 0.841 0.727 0.34  0.071 0.377 0.205 0.931 0.733 0.04 ]\n",
      "Agents Guess Best Lever :  7\n"
     ]
    }
   ],
   "source": [
    "print('\\n-------------------------')\n",
    "print(\"Reward Over Time Tracking : \", reward_count) \n",
    "print(\"Agent's weight : \", agents_prob)\n",
    "print(\"Agent's Probability Guess : \", reward_count/number_of_levers_count )\n",
    "print(\"Agents Guess Best Lever : \", \n",
    "      np.argmax(reward_count/number_of_levers_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------\n",
      "Probability of each lever Ground Truth Full list :  [0.546 0.859 0.686 0.332 0.06  0.386 0.213 0.933 0.723 0.047]\n",
      "Ground Truth Best Lever :  7\n"
     ]
    }
   ],
   "source": [
    "print('\\n-------------------------')\n",
    "print(\"Probability of each lever Ground Truth Full list : \", \n",
    "      ground_truth_prob)\n",
    "print(\"Ground Truth Best Lever : \", np.argmax(ground_truth_prob))\n",
    "\n",
    "# -- end code --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Gradient with Multi Layer Perceptron Network\n",
    "\n",
    "we can create two fully connected network, and using this method we can expand the Log-Likelihood method, rather than directly getting which bandit we want to pull, we can train a neural network that computes the optimal bandit by using back propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_sig(x): return 1 / (1 + np.exp(-x))\n",
    "def d_log_sig(x): return log_sig(x) * ( 1 - log_sig(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_prob = np.random.rand(10)\n",
    "number_of_levers_count = np.zeros(10)\n",
    "\n",
    "agents_prob  = np.zeros(10)\n",
    "reward_count = np.zeros(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episode = 20000\n",
    "e = 0.1\n",
    "learning_rate = 0.00000000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN():\n",
    "    \n",
    "    def __init__(self,inc,outc):\n",
    "        self.w = np.random.randn(inc,outc) \n",
    "\n",
    "    def feed(self, input):\n",
    "        self.input = input\n",
    "        self.layer = np.dot(self.input,self.w)\n",
    "        self.layerA = log_sig(self.layer)\n",
    "        return self.layerA\n",
    "    \n",
    "    def back(self, grad):\n",
    "        grad_part_1 = grad\n",
    "        grad_part_2 = d_log_sig(self.layer)\n",
    "        grad_part_3 = self.input\n",
    "\n",
    "        grad_middle = grad_part_1 * grad_part_2\n",
    "        grad = grad_part_3.T.dot(grad_middle)\n",
    "        grad_pass = grad_middle.dot(self.w.T)\n",
    "        self.w = self.w - learning_rate * grad\n",
    "        return grad_pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = FNN(10,30)\n",
    "l2 = FNN(30,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(num_episode):\n",
    "    \n",
    "    # Either choose a greedy action or explore\n",
    "    if e > np.random.uniform():\n",
    "        which_lever_to_pull = np.random.randint(0,10)\n",
    "    else:\n",
    "        which_lever_to_pull = np.argmax(agents_prob)\n",
    "\n",
    "    temp = np.zeros((1,10))\n",
    "    temp[0,which_lever_to_pull] = 1\n",
    "    layer1 = l1.feed(temp)\n",
    "    layer2 = l2.feed(layer1)\n",
    "    which_lever_to_pull2 = np.argmax(layer2)\n",
    "\n",
    "    # now pull the lever \n",
    "    if ground_truth_prob[which_lever_to_pull2] > np.random.uniform():\n",
    "        reward = 1\n",
    "    else:\n",
    "        reward = 0\n",
    "\n",
    "    # now update the lever count and expected value\n",
    "    number_of_levers_count[which_lever_to_pull2] += 1\n",
    "    loss =  - np.log( np.clip(agents_prob * temp,1e-10,1e10)  ) * reward \n",
    "    back_prop = (-1/ np.clip(agents_prob * temp,1e-10,1e10)  )* reward\n",
    "    grad2 = l2.back(back_prop)\n",
    "    grad1 = l1.back(grad2)\n",
    "    agents_prob = agents_prob - learning_rate * grad1 \n",
    "    reward_count[which_lever_to_pull2] = reward_count[which_lever_to_pull2]\\\n",
    "                                        + reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------\n",
      "Number of Levers Count Each:  [   99.   254.     0. 17597.   213.     0.   183.  1247.   191.   216.]\n",
      "Sum of lever must match with # episode :  20000.0  :  20000\n"
     ]
    }
   ],
   "source": [
    "print('\\n-------------------------')\n",
    "print(\"Number of Levers Count Each: \", number_of_levers_count)\n",
    "print(\"Sum of lever must match with # episode : \",number_of_levers_count.sum(), ' : ',\n",
    "      num_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------\n",
      "Reward Over Time Tracking :  [  33.   76.    0. 1700.   52.    0.   44.  123.   23.  164.]\n",
      "Agent's weight :  [[ 4.587  3.857  3.427  3.64  -2.069  1.604  4.651  5.307  0.077  7.685]]\n",
      "Agent's Probability Guess :  [0.333 0.299 0.    0.097 0.244 0.    0.24  0.099 0.12  0.759]\n",
      "Agents Guess Best Lever :  9\n"
     ]
    }
   ],
   "source": [
    "print('\\n-------------------------')\n",
    "print(\"Reward Over Time Tracking : \", reward_count)\n",
    "print(\"Agent's weight : \", agents_prob)\n",
    "print(\"Agent's Probability Guess : \", \n",
    "      reward_count/(number_of_levers_count+1e-10))\n",
    "print(\"Agents Guess Best Lever : \", \n",
    "      np.argmax(reward_count/(number_of_levers_count+1e-10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------\n",
      "Probability of each lever Ground Truth Full list :  [0.398 0.264 0.578 0.096 0.212 0.957 0.235 0.1   0.144 0.711]\n",
      "Ground Truth Best Lever :  5\n"
     ]
    }
   ],
   "source": [
    "print('\\n-------------------------')\n",
    "print(\"Probability of each lever Ground Truth Full list : \", \n",
    "      ground_truth_prob)\n",
    "print(\"Ground Truth Best Lever : \", np.argmax(ground_truth_prob))\n",
    "\n",
    "# -- end code --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Reward Inaction\n",
    "\n",
    "This method is simpler than using a neural network, we are simply going to increase the probability of getting the optimal bandit to be choose in the given probability distribution. We are only going to update when we receive a reward from the bandit. And when we do not have any reward we aren’t going to update anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_prob = np.random.rand(10)\n",
    "number_of_levers_count = np.zeros(10)\n",
    "\n",
    "agents_prob = np.ones(10) * 0.1\n",
    "reward_count = np.zeros(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episode = 40000\n",
    "e = 0.33\n",
    "alpha = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(num_episode):\n",
    "    \n",
    "    # Either choose a greedy action or explore\n",
    "    if e > np.random.uniform():\n",
    "        which_lever_to_pull = np.random.randint(0,10)\n",
    "    else:\n",
    "        which_lever_to_pull = np.random.choice(10, p=agents_prob)\n",
    "\n",
    "    # now pull the lever \n",
    "    if ground_truth_prob[which_lever_to_pull] > np.random.uniform():\n",
    "        reward = 1\n",
    "    else:\n",
    "        reward = 0\n",
    "\n",
    "    # now update the lever count and expected value\n",
    "    number_of_levers_count[which_lever_to_pull] += 1\n",
    "    reward_count[which_lever_to_pull] = reward_count[which_lever_to_pull] \\ \n",
    "                                        + reward\n",
    "    if reward == 1:\n",
    "        mask1 = np.zeros(10)\n",
    "        mask2 = np.ones(10)\n",
    "        mask1[which_lever_to_pull] = 1\n",
    "        mask2[which_lever_to_pull] = 0\n",
    "        lever_won_vector = (agents_prob + alpha * (1- agents_prob)) \\\n",
    "                            * mask1\n",
    "        rest_of_levers   = ((1-alpha) * agents_prob) * mask2\n",
    "        agents_prob = lever_won_vector + rest_of_levers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------\n",
      "Number of Levers Count Each:  [2120. 3006. 4534. 4480. 3147. 6465. 2886. 7687. 2558. 3117.]\n",
      "Sum of lever must match with # episode :  40000.0  :  40000\n"
     ]
    }
   ],
   "source": [
    "print('\\n-------------------------')\n",
    "print(\"Number of Levers Count Each: \", number_of_levers_count)\n",
    "print(\"Sum of lever must match with # episode : \",\n",
    "      number_of_levers_count.sum(),' : ', num_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------\n",
      "Reward Over Time Tracking :  [ 867. 1726. 3366. 3339. 1919. 5667. 1628. 7267. 1331. 1733.]\n",
      "Agent's weight :  [0.264 0.371 0.008 0.168 0.    0.182 0.    0.008 0.    0.   ]\n",
      "Agent's Probability Guess :  [0.409 0.574 0.742 0.745 0.61  0.877 0.564 0.945 0.52  0.556]\n",
      "Agents Guess Best Lever :  7\n"
     ]
    }
   ],
   "source": [
    "print('\\n-------------------------')\n",
    "print(\"Reward Over Time Tracking : \", reward_count)\n",
    "print(\"Agent's weight : \", agents_prob)\n",
    "print(\"Agent's Probability Guess : \", reward_count/number_of_levers_count )\n",
    "print(\"Agents Guess Best Lever : \", np.argmax(reward_count/number_of_levers_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------\n",
      "Ground Truth Full list :  [0.412 0.589 0.742 0.736 0.606 0.878 0.56  0.944 0.513 0.555]\n",
      "Ground Truth Best Lever :  7\n"
     ]
    }
   ],
   "source": [
    "print('\\n-------------------------')\n",
    "print(\"Ground Truth Full list : \", ground_truth_prob)\n",
    "print(\"Ground Truth Best Lever : \", np.argmax(ground_truth_prob))\n",
    "\n",
    "# -- end code --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Reward Penalty\n",
    "\n",
    "W simply expand the Linear Reward Inaction method, as seen above, when we do not get a reward for certain bandit. We are simply going to decrease the probability of that bandit being chosen in the next round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_prob = np.random.rand(10)\n",
    "number_of_levers_count = np.zeros(10)\n",
    "agents_prob = np.ones(10) * 0.1\n",
    "reward_count = np.zeros(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episode = 40000\n",
    "e = 0.33\n",
    "alpha = 0.2\n",
    "beta = 0.8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(num_episode):\n",
    "    \n",
    "    # Either choose a greedy action or explore\n",
    "    if e > np.random.uniform():\n",
    "        which_lever_to_pull = np.random.randint(0,10)\n",
    "    else:\n",
    "        which_lever_to_pull = np.random.choice(10, p=agents_prob)\n",
    "\n",
    "    # now pull the lever \n",
    "    if ground_truth_prob[which_lever_to_pull] > np.random.uniform():\n",
    "        reward = 1\n",
    "    else:\n",
    "        reward = 0\n",
    "\n",
    "    # now update the lever count and expected value\n",
    "    number_of_levers_count[which_lever_to_pull] += 1\n",
    "    reward_count[which_lever_to_pull] = reward_count[which_lever_to_pull] \\\n",
    "                                        + reward\n",
    "    mask1 = np.zeros(10)\n",
    "    mask2 = np.ones(10)\n",
    "    mask1[which_lever_to_pull] = 1\n",
    "    mask2[which_lever_to_pull] = 0\n",
    "    if reward == 1:\n",
    "        lever_won_vector = (agents_prob + alpha * (1- agents_prob)) \\\n",
    "                           * mask1\n",
    "        rest_of_levers   = ((1-alpha) * agents_prob) * mask2\n",
    "        agents_prob = lever_won_vector + rest_of_levers\n",
    "    else:\n",
    "        rest_of_levers = ( beta / (len(ground_truth_prob)-1) \\\n",
    "                         + (1-beta) *agents_prob ) * mask2\n",
    "        lever_lose_vector = ( (1-beta)*agents_prob )* mask1\n",
    "        agents_prob = lever_lose_vector + rest_of_levers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------\n",
      "Number of Levers Count Each:  [4002. 4946. 4369. 4667. 3397. 3952. 4274. 4038. 3247. 3108.]\n",
      "Sum of lever must match with # episode :  40000.0  :  40000\n"
     ]
    }
   ],
   "source": [
    "print('\\n-------------------------')\n",
    "print(\"Number of Levers Count Each: \", number_of_levers_count)\n",
    "print(\"Sum of lever must match with # episode : \",number_of_levers_count.sum(), ' : ', num_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------\n",
      "Reward Over Time Tracking :  [3143. 4866. 3981. 4545. 1776. 3204. 3473. 3076. 1758. 1365.]\n",
      "Agent's weight :  [0.009 0.01  0.01  0.137 0.073 0.002 0.009 0.53  0.21  0.009]\n",
      "Agent's Probability Guess :  [0.785 0.984 0.911 0.974 0.523 0.811 0.813 0.762 0.541 0.439]\n",
      "Agents Guess Best Lever :  1\n"
     ]
    }
   ],
   "source": [
    "print('\\n-------------------------')\n",
    "print(\"Reward Over Time Tracking : \", reward_count)\n",
    "print(\"Agent's weight : \", agents_prob)\n",
    "print(\"Agent's Probability Guess : \", reward_count/number_of_levers_count )\n",
    "print(\"Agents Guess Best Lever : \", np.argmax(reward_count/number_of_levers_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------\n",
      "Ground Truth Full list :  [0.793 0.984 0.907 0.973 0.531 0.823 0.816 0.751 0.547 0.436]\n",
      "Ground Truth Best Lever :  1\n"
     ]
    }
   ],
   "source": [
    "print('\\n-------------------------')\n",
    "print(\"Ground Truth Full list : \", ground_truth_prob)\n",
    "print(\"Ground Truth Best Lever : \", np.argmax(ground_truth_prob))\n",
    "\n",
    "# -- end code --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ad CTR Optimization using Multi-armed Bandit Aproach\n",
    "\n",
    "Suppose an advertising company is running 10 different ads targeted towards a similar set of population on a webpage. We have a 1 if the ad was clicked by a user, and 0 if it was not, for each Ad.\n",
    "\n",
    "Obtained data from https://drive.google.com/file/d/1whkIInL4FKeHg2IfdcbT1j18L26fg9aF/view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Selection\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv('/Users/chrisjcc/Downloads/Ads_Optimisation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Random Selection\n",
    "import random\n",
    "N = 10000\n",
    "d = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ads_selected = []\n",
    "total_reward = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(0, N):\n",
    "    ad = random.randrange(d)\n",
    "    ads_selected.append(ad)\n",
    "    reward = dataset.values[n, ad]\n",
    "    total_reward = total_reward + reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7    0.118\n",
       "8    0.111\n",
       "6    0.106\n",
       "0    0.104\n",
       "2    0.102\n",
       "9    0.099\n",
       "3    0.099\n",
       "4    0.094\n",
       "1    0.086\n",
       "5    0.081\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(ads_selected).tail(1000).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total reward for the random selection algorithm comes out to be 1170. As this algorithm is not learning anything, it will not smartly select any ad which is giving the maximum return. And hence even if we look at the last 1000 trials, it is not able to find the optimal ad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Upper Confidence Bound (UCB)\n",
    "import math\n",
    "N = 10000\n",
    "d = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ads_selected = []\n",
    "numbers_of_selections = [0] * d\n",
    "sums_of_reward = [0] * d\n",
    "total_reward = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(0, N):\n",
    "    ad = 0\n",
    "    max_upper_bound = 0\n",
    "    for i in range(0, d):\n",
    "        if (numbers_of_selections[i] > 0):\n",
    "            average_reward = sums_of_reward[i] / numbers_of_selections[i]\n",
    "            delta_i = math.sqrt(2 * math.log(n+1) / numbers_of_selections[i])\n",
    "            upper_bound = average_reward + delta_i\n",
    "        else:\n",
    "            upper_bound = 1e400\n",
    "        if upper_bound > max_upper_bound:\n",
    "            max_upper_bound = upper_bound\n",
    "            ad = i\n",
    "    ads_selected.append(ad)\n",
    "    numbers_of_selections[ad] += 1\n",
    "    reward = dataset.values[n, ad]\n",
    "    sums_of_reward[ad] += reward\n",
    "    total_reward += reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7    0.192\n",
       "4    0.153\n",
       "0    0.115\n",
       "8    0.096\n",
       "3    0.093\n",
       "6    0.080\n",
       "1    0.080\n",
       "2    0.077\n",
       "5    0.059\n",
       "9    0.055\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(ads_selected).head(1000).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total_reward for UCB comes out to be 2125. Clearly, this is much better than random selection and indeed a smart exploration technique that can significantly improve our strategy to solve a MABP.\n",
    "\n",
    "After just 1500 trials, UCB is already favouring Ad #5 (index 4) which happens to be the optimal ad, and gets the maximum return for the given problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
