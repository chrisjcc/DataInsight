{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving FrozenLake-v0 with Reinforcement Learning\n",
    "\n",
    "\n",
    "    # LEFT = 0 DOWN = 1 RIGHT = 2 UP = 3\n",
    "    # SFFF\n",
    "    # FHFH\n",
    "    # FFFH\n",
    "    # HFFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chrisjcc/.virtualenvs/ml/lib/python3.7/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "# FrozenLake-v0 Game\n",
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set game configuration parameters\n",
    "n_games = 1000 # play 1k games\n",
    "win_pct = []\n",
    "scores = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent takes andom actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through 1000 games\n",
    "for i in range(n_games):\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    score = 0\n",
    "    \n",
    "    while not done: # loop through episodes\n",
    "        action = env.action_space.sample() # sample random action\n",
    "        obs, reward, done, info = env.step(action) # steping through the env\n",
    "        score += reward\n",
    "        \n",
    "    # at the end of every eposide we append the scores collected by agent\n",
    "    scores.append(score)\n",
    "    \n",
    "    if i % 10 == 0 :\n",
    "        # every 10 games we want to keep track of the average win percentage\n",
    "        average = np.mean(scores[-10:]) \n",
    "        win_pct.append(average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot win percentage after all the games\n",
    "plt.plot(win_pct)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the Agent with random action perform rather poorly. The majority of the time the agent falls in a hole and occasionally does make it out by sheer random luck. The maximum win percentage we observe is around 10% and varies from run to run some manage to achieve as high as 20%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next agent takes deterministic policy\n",
    "\n",
    "To represent out policy we can use a dictionary, where the keys correspond to the states while the valeus correspond to the actions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one possible determistic policy we can draft\n",
    "policy = {0: 1, 1: 2, 2: 1, 3: 0, 4: 1, 6: 1, 8: 2, 9: 1, 10: 1, 13: 2, 14: 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set game configuration parameters\n",
    "n_games = 1000 # play 1k games\n",
    "win_pct = []\n",
    "scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through 1000 games\n",
    "for i in range(n_games):\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    score = 0\n",
    "    \n",
    "    while not done: # loop through episodes\n",
    "        action = policy[obs] # we use our deterministic policy\n",
    "        obs, reward, done, info = env.step(action) # steping through the env\n",
    "        score += reward\n",
    "        \n",
    "    # at the end of every eposide we append the scores collected by agent\n",
    "    scores.append(score)\n",
    "    \n",
    "    if i % 10 == 0 :\n",
    "        # every 10 games we want to keep track of the average win percentage\n",
    "        average = np.mean(scores[-10:]) \n",
    "        win_pct.append(average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot win percentage after all the games\n",
    "plt.plot(win_pct)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we get a frequent win rate of 20% doingslightly better then our previous random action Agent. The majority of the time the Agent falls in the hole, and some of the time the Agent manages to make it out 1 out of every 10 times. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent uses Q-learning algorithm aproach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, lr, gamma, n_actions, n_states, \n",
    "                  eps_start, eps_end, eps_dec):\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.n_actions = n_actions\n",
    "        self.n_states  = n_states\n",
    "        self.epsilon   = eps_start\n",
    "        self.eps_min   = eps_end\n",
    "        self.eps_dec   = eps_dec\n",
    "        \n",
    "        self.Q = {}\n",
    "        \n",
    "        self.init_Q()\n",
    "        \n",
    "    def init_Q(self):\n",
    "        for state in range(self.n_states): # rows\n",
    "            for action in range(self.n_actions): # columns\n",
    "                self.Q[(state, action)] = 0.0\n",
    "        \n",
    "    def choose_action(self, state):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            # random action\n",
    "            action = np.random.choice([i for i in range(self.n_actions)])\n",
    "        else:\n",
    "            actions = np.array([self.Q[(state, a)] for a in range(self.n_actions)])\n",
    "            action  = np.argmax(actions) # greedy action (for actions that ties the action lowest index is returned)\n",
    "                \n",
    "        return action\n",
    "        \n",
    "    def decrement_epsilon(self): # linear decrement\n",
    "        self.epsilon = self.epsilon * self.eps_dec if self.epsilon > self.eps_min else self.eps_min\n",
    "            \n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        actions = np.array([self.Q[(next_state, a)] for a in range(self.n_actions)])\n",
    "        a_max = np.argmax(actions)\n",
    "        \n",
    "        # update table of Q-values estimates\n",
    "        self.Q[(state, action)] += self.lr * (reward + self.gamma * self.Q[(next_state, a_max)] - self.Q[(state, action)])\n",
    "        \n",
    "        # decrease the epsilon-greedy action selection\n",
    "        self.decrement_epsilon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(lr=0.001, gamma=0.9, eps_start=1.0, eps_end=0.01, \n",
    "             eps_dec=0.9999995, n_actions=4, n_states=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set game configuration parameters\n",
    "win_pct_list = []\n",
    "scores = []\n",
    "n_games = 500000 # play 500k games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  0 win pct 0.00 episode 1.00\n",
      "episode  1000 win pct 0.02 episode 1.00\n",
      "episode  2000 win pct 0.02 episode 0.99\n",
      "episode  3000 win pct 0.01 episode 0.99\n",
      "episode  4000 win pct 0.02 episode 0.98\n",
      "episode  5000 win pct 0.03 episode 0.98\n",
      "episode  6000 win pct 0.02 episode 0.98\n",
      "episode  7000 win pct 0.02 episode 0.97\n",
      "episode  8000 win pct 0.01 episode 0.97\n",
      "episode  9000 win pct 0.01 episode 0.97\n",
      "episode  10000 win pct 0.01 episode 0.96\n",
      "episode  11000 win pct 0.02 episode 0.96\n",
      "episode  12000 win pct 0.02 episode 0.95\n",
      "episode  13000 win pct 0.01 episode 0.95\n",
      "episode  14000 win pct 0.02 episode 0.95\n",
      "episode  15000 win pct 0.00 episode 0.94\n",
      "episode  16000 win pct 0.00 episode 0.94\n",
      "episode  17000 win pct 0.03 episode 0.94\n",
      "episode  18000 win pct 0.01 episode 0.93\n",
      "episode  19000 win pct 0.01 episode 0.93\n",
      "episode  20000 win pct 0.01 episode 0.93\n",
      "episode  21000 win pct 0.01 episode 0.92\n",
      "episode  22000 win pct 0.02 episode 0.92\n",
      "episode  23000 win pct 0.00 episode 0.91\n",
      "episode  24000 win pct 0.01 episode 0.91\n",
      "episode  25000 win pct 0.02 episode 0.91\n",
      "episode  26000 win pct 0.01 episode 0.90\n",
      "episode  27000 win pct 0.00 episode 0.90\n",
      "episode  28000 win pct 0.02 episode 0.90\n",
      "episode  29000 win pct 0.01 episode 0.89\n",
      "episode  30000 win pct 0.01 episode 0.89\n",
      "episode  31000 win pct 0.01 episode 0.89\n",
      "episode  32000 win pct 0.03 episode 0.88\n",
      "episode  33000 win pct 0.01 episode 0.88\n",
      "episode  34000 win pct 0.00 episode 0.87\n",
      "episode  35000 win pct 0.01 episode 0.87\n",
      "episode  36000 win pct 0.01 episode 0.87\n",
      "episode  37000 win pct 0.04 episode 0.86\n",
      "episode  38000 win pct 0.01 episode 0.86\n",
      "episode  39000 win pct 0.02 episode 0.86\n",
      "episode  40000 win pct 0.01 episode 0.85\n",
      "episode  41000 win pct 0.04 episode 0.85\n",
      "episode  42000 win pct 0.02 episode 0.85\n",
      "episode  43000 win pct 0.01 episode 0.84\n",
      "episode  44000 win pct 0.02 episode 0.84\n",
      "episode  45000 win pct 0.02 episode 0.83\n",
      "episode  46000 win pct 0.04 episode 0.83\n",
      "episode  47000 win pct 0.03 episode 0.83\n",
      "episode  48000 win pct 0.02 episode 0.82\n",
      "episode  49000 win pct 0.02 episode 0.82\n",
      "episode  50000 win pct 0.03 episode 0.82\n",
      "episode  51000 win pct 0.01 episode 0.81\n",
      "episode  52000 win pct 0.04 episode 0.81\n",
      "episode  53000 win pct 0.03 episode 0.81\n",
      "episode  54000 win pct 0.04 episode 0.80\n",
      "episode  55000 win pct 0.02 episode 0.80\n",
      "episode  56000 win pct 0.02 episode 0.80\n",
      "episode  57000 win pct 0.03 episode 0.79\n",
      "episode  58000 win pct 0.03 episode 0.79\n",
      "episode  59000 win pct 0.03 episode 0.79\n",
      "episode  60000 win pct 0.03 episode 0.78\n",
      "episode  61000 win pct 0.02 episode 0.78\n",
      "episode  62000 win pct 0.01 episode 0.78\n",
      "episode  63000 win pct 0.03 episode 0.77\n",
      "episode  64000 win pct 0.03 episode 0.77\n",
      "episode  65000 win pct 0.02 episode 0.77\n",
      "episode  66000 win pct 0.04 episode 0.76\n",
      "episode  67000 win pct 0.02 episode 0.76\n",
      "episode  68000 win pct 0.04 episode 0.76\n",
      "episode  69000 win pct 0.02 episode 0.75\n",
      "episode  70000 win pct 0.05 episode 0.75\n",
      "episode  71000 win pct 0.01 episode 0.75\n",
      "episode  72000 win pct 0.00 episode 0.74\n",
      "episode  73000 win pct 0.04 episode 0.74\n",
      "episode  74000 win pct 0.04 episode 0.74\n",
      "episode  75000 win pct 0.02 episode 0.73\n",
      "episode  76000 win pct 0.04 episode 0.73\n",
      "episode  77000 win pct 0.02 episode 0.73\n",
      "episode  78000 win pct 0.02 episode 0.72\n",
      "episode  79000 win pct 0.01 episode 0.72\n",
      "episode  80000 win pct 0.03 episode 0.72\n",
      "episode  81000 win pct 0.05 episode 0.71\n",
      "episode  82000 win pct 0.04 episode 0.71\n",
      "episode  83000 win pct 0.03 episode 0.71\n",
      "episode  84000 win pct 0.02 episode 0.70\n",
      "episode  85000 win pct 0.06 episode 0.70\n",
      "episode  86000 win pct 0.03 episode 0.70\n",
      "episode  87000 win pct 0.03 episode 0.69\n",
      "episode  88000 win pct 0.06 episode 0.69\n",
      "episode  89000 win pct 0.01 episode 0.69\n",
      "episode  90000 win pct 0.00 episode 0.69\n",
      "episode  91000 win pct 0.03 episode 0.68\n",
      "episode  92000 win pct 0.02 episode 0.68\n",
      "episode  93000 win pct 0.04 episode 0.68\n",
      "episode  94000 win pct 0.01 episode 0.67\n",
      "episode  95000 win pct 0.02 episode 0.67\n",
      "episode  96000 win pct 0.06 episode 0.67\n",
      "episode  97000 win pct 0.02 episode 0.66\n",
      "episode  98000 win pct 0.02 episode 0.66\n",
      "episode  99000 win pct 0.02 episode 0.66\n",
      "episode  100000 win pct 0.02 episode 0.65\n",
      "episode  101000 win pct 0.05 episode 0.65\n",
      "episode  102000 win pct 0.03 episode 0.65\n",
      "episode  103000 win pct 0.04 episode 0.64\n",
      "episode  104000 win pct 0.01 episode 0.64\n",
      "episode  105000 win pct 0.03 episode 0.64\n",
      "episode  106000 win pct 0.04 episode 0.63\n",
      "episode  107000 win pct 0.04 episode 0.63\n",
      "episode  108000 win pct 0.04 episode 0.63\n",
      "episode  109000 win pct 0.01 episode 0.63\n",
      "episode  110000 win pct 0.04 episode 0.62\n",
      "episode  111000 win pct 0.00 episode 0.62\n",
      "episode  112000 win pct 0.03 episode 0.62\n",
      "episode  113000 win pct 0.05 episode 0.61\n",
      "episode  114000 win pct 0.06 episode 0.61\n",
      "episode  115000 win pct 0.05 episode 0.61\n",
      "episode  116000 win pct 0.05 episode 0.60\n",
      "episode  117000 win pct 0.02 episode 0.60\n",
      "episode  118000 win pct 0.05 episode 0.60\n",
      "episode  119000 win pct 0.03 episode 0.60\n",
      "episode  120000 win pct 0.03 episode 0.59\n",
      "episode  121000 win pct 0.06 episode 0.59\n",
      "episode  122000 win pct 0.02 episode 0.59\n",
      "episode  123000 win pct 0.06 episode 0.58\n",
      "episode  124000 win pct 0.06 episode 0.58\n",
      "episode  125000 win pct 0.08 episode 0.58\n",
      "episode  126000 win pct 0.07 episode 0.57\n",
      "episode  127000 win pct 0.06 episode 0.57\n",
      "episode  128000 win pct 0.02 episode 0.57\n",
      "episode  129000 win pct 0.01 episode 0.56\n",
      "episode  130000 win pct 0.04 episode 0.56\n",
      "episode  131000 win pct 0.04 episode 0.56\n",
      "episode  132000 win pct 0.04 episode 0.55\n",
      "episode  133000 win pct 0.07 episode 0.55\n",
      "episode  134000 win pct 0.05 episode 0.55\n",
      "episode  135000 win pct 0.08 episode 0.54\n",
      "episode  136000 win pct 0.06 episode 0.54\n",
      "episode  137000 win pct 0.03 episode 0.54\n",
      "episode  138000 win pct 0.11 episode 0.53\n",
      "episode  139000 win pct 0.10 episode 0.53\n",
      "episode  140000 win pct 0.09 episode 0.53\n",
      "episode  141000 win pct 0.05 episode 0.53\n",
      "episode  142000 win pct 0.05 episode 0.52\n",
      "episode  143000 win pct 0.07 episode 0.52\n",
      "episode  144000 win pct 0.05 episode 0.52\n",
      "episode  145000 win pct 0.05 episode 0.51\n",
      "episode  146000 win pct 0.06 episode 0.51\n",
      "episode  147000 win pct 0.04 episode 0.51\n",
      "episode  148000 win pct 0.08 episode 0.50\n",
      "episode  149000 win pct 0.06 episode 0.50\n",
      "episode  150000 win pct 0.09 episode 0.50\n",
      "episode  151000 win pct 0.05 episode 0.49\n",
      "episode  152000 win pct 0.06 episode 0.49\n",
      "episode  153000 win pct 0.06 episode 0.49\n",
      "episode  154000 win pct 0.05 episode 0.48\n",
      "episode  155000 win pct 0.10 episode 0.48\n",
      "episode  156000 win pct 0.08 episode 0.48\n",
      "episode  157000 win pct 0.06 episode 0.47\n",
      "episode  158000 win pct 0.08 episode 0.47\n",
      "episode  159000 win pct 0.08 episode 0.47\n",
      "episode  160000 win pct 0.08 episode 0.47\n",
      "episode  161000 win pct 0.08 episode 0.46\n",
      "episode  162000 win pct 0.11 episode 0.46\n",
      "episode  163000 win pct 0.08 episode 0.46\n",
      "episode  164000 win pct 0.05 episode 0.45\n",
      "episode  165000 win pct 0.11 episode 0.45\n",
      "episode  166000 win pct 0.10 episode 0.45\n",
      "episode  167000 win pct 0.07 episode 0.44\n",
      "episode  168000 win pct 0.10 episode 0.44\n",
      "episode  169000 win pct 0.07 episode 0.44\n",
      "episode  170000 win pct 0.10 episode 0.44\n",
      "episode  171000 win pct 0.07 episode 0.43\n",
      "episode  172000 win pct 0.06 episode 0.43\n",
      "episode  173000 win pct 0.14 episode 0.43\n",
      "episode  174000 win pct 0.11 episode 0.42\n",
      "episode  175000 win pct 0.09 episode 0.42\n",
      "episode  176000 win pct 0.12 episode 0.42\n",
      "episode  177000 win pct 0.13 episode 0.41\n",
      "episode  178000 win pct 0.06 episode 0.41\n",
      "episode  179000 win pct 0.07 episode 0.41\n",
      "episode  180000 win pct 0.11 episode 0.41\n",
      "episode  181000 win pct 0.09 episode 0.40\n",
      "episode  182000 win pct 0.08 episode 0.40\n",
      "episode  183000 win pct 0.15 episode 0.40\n",
      "episode  184000 win pct 0.11 episode 0.39\n",
      "episode  185000 win pct 0.09 episode 0.39\n",
      "episode  186000 win pct 0.11 episode 0.39\n",
      "episode  187000 win pct 0.13 episode 0.38\n",
      "episode  188000 win pct 0.13 episode 0.38\n",
      "episode  189000 win pct 0.15 episode 0.38\n",
      "episode  190000 win pct 0.11 episode 0.38\n",
      "episode  191000 win pct 0.09 episode 0.37\n",
      "episode  192000 win pct 0.13 episode 0.37\n",
      "episode  193000 win pct 0.13 episode 0.37\n",
      "episode  194000 win pct 0.13 episode 0.36\n",
      "episode  195000 win pct 0.20 episode 0.36\n",
      "episode  196000 win pct 0.08 episode 0.36\n",
      "episode  197000 win pct 0.11 episode 0.36\n",
      "episode  198000 win pct 0.16 episode 0.35\n",
      "episode  199000 win pct 0.15 episode 0.35\n",
      "episode  200000 win pct 0.12 episode 0.35\n",
      "episode  201000 win pct 0.14 episode 0.34\n",
      "episode  202000 win pct 0.12 episode 0.34\n",
      "episode  203000 win pct 0.11 episode 0.34\n",
      "episode  204000 win pct 0.12 episode 0.34\n",
      "episode  205000 win pct 0.13 episode 0.33\n",
      "episode  206000 win pct 0.12 episode 0.33\n",
      "episode  207000 win pct 0.09 episode 0.33\n",
      "episode  208000 win pct 0.18 episode 0.33\n",
      "episode  209000 win pct 0.21 episode 0.32\n",
      "episode  210000 win pct 0.12 episode 0.32\n",
      "episode  211000 win pct 0.22 episode 0.32\n",
      "episode  212000 win pct 0.17 episode 0.31\n",
      "episode  213000 win pct 0.13 episode 0.31\n",
      "episode  214000 win pct 0.10 episode 0.31\n",
      "episode  215000 win pct 0.11 episode 0.31\n",
      "episode  216000 win pct 0.11 episode 0.30\n",
      "episode  217000 win pct 0.18 episode 0.30\n",
      "episode  218000 win pct 0.11 episode 0.30\n",
      "episode  219000 win pct 0.13 episode 0.30\n",
      "episode  220000 win pct 0.14 episode 0.29\n",
      "episode  221000 win pct 0.15 episode 0.29\n",
      "episode  222000 win pct 0.21 episode 0.29\n",
      "episode  223000 win pct 0.16 episode 0.29\n",
      "episode  224000 win pct 0.13 episode 0.28\n",
      "episode  225000 win pct 0.19 episode 0.28\n",
      "episode  226000 win pct 0.18 episode 0.28\n",
      "episode  227000 win pct 0.15 episode 0.28\n",
      "episode  228000 win pct 0.18 episode 0.27\n",
      "episode  229000 win pct 0.23 episode 0.27\n",
      "episode  230000 win pct 0.21 episode 0.27\n",
      "episode  231000 win pct 0.23 episode 0.27\n",
      "episode  232000 win pct 0.25 episode 0.26\n",
      "episode  233000 win pct 0.27 episode 0.26\n",
      "episode  234000 win pct 0.16 episode 0.26\n",
      "episode  235000 win pct 0.17 episode 0.26\n",
      "episode  236000 win pct 0.15 episode 0.25\n",
      "episode  237000 win pct 0.15 episode 0.25\n",
      "episode  238000 win pct 0.19 episode 0.25\n",
      "episode  239000 win pct 0.18 episode 0.25\n",
      "episode  240000 win pct 0.23 episode 0.24\n",
      "episode  241000 win pct 0.16 episode 0.24\n",
      "episode  242000 win pct 0.19 episode 0.24\n",
      "episode  243000 win pct 0.18 episode 0.24\n",
      "episode  244000 win pct 0.26 episode 0.23\n",
      "episode  245000 win pct 0.20 episode 0.23\n",
      "episode  246000 win pct 0.27 episode 0.23\n",
      "episode  247000 win pct 0.18 episode 0.23\n",
      "episode  248000 win pct 0.22 episode 0.22\n",
      "episode  249000 win pct 0.15 episode 0.22\n",
      "episode  250000 win pct 0.25 episode 0.22\n",
      "episode  251000 win pct 0.23 episode 0.22\n",
      "episode  252000 win pct 0.19 episode 0.22\n",
      "episode  253000 win pct 0.24 episode 0.21\n",
      "episode  254000 win pct 0.28 episode 0.21\n",
      "episode  255000 win pct 0.26 episode 0.21\n",
      "episode  256000 win pct 0.33 episode 0.21\n",
      "episode  257000 win pct 0.13 episode 0.20\n",
      "episode  258000 win pct 0.26 episode 0.20\n",
      "episode  259000 win pct 0.27 episode 0.20\n",
      "episode  260000 win pct 0.29 episode 0.20\n",
      "episode  261000 win pct 0.29 episode 0.20\n",
      "episode  262000 win pct 0.24 episode 0.19\n",
      "episode  263000 win pct 0.26 episode 0.19\n",
      "episode  264000 win pct 0.26 episode 0.19\n",
      "episode  265000 win pct 0.29 episode 0.19\n",
      "episode  266000 win pct 0.27 episode 0.19\n",
      "episode  267000 win pct 0.30 episode 0.18\n",
      "episode  268000 win pct 0.32 episode 0.18\n",
      "episode  269000 win pct 0.28 episode 0.18\n",
      "episode  270000 win pct 0.28 episode 0.18\n",
      "episode  271000 win pct 0.25 episode 0.17\n",
      "episode  272000 win pct 0.29 episode 0.17\n",
      "episode  273000 win pct 0.33 episode 0.17\n",
      "episode  274000 win pct 0.29 episode 0.17\n",
      "episode  275000 win pct 0.29 episode 0.17\n",
      "episode  276000 win pct 0.20 episode 0.17\n",
      "episode  277000 win pct 0.16 episode 0.16\n",
      "episode  278000 win pct 0.24 episode 0.16\n",
      "episode  279000 win pct 0.32 episode 0.16\n",
      "episode  280000 win pct 0.29 episode 0.16\n",
      "episode  281000 win pct 0.25 episode 0.16\n",
      "episode  282000 win pct 0.25 episode 0.15\n",
      "episode  283000 win pct 0.29 episode 0.15\n",
      "episode  284000 win pct 0.29 episode 0.15\n",
      "episode  285000 win pct 0.30 episode 0.15\n",
      "episode  286000 win pct 0.40 episode 0.15\n",
      "episode  287000 win pct 0.37 episode 0.14\n",
      "episode  288000 win pct 0.34 episode 0.14\n",
      "episode  289000 win pct 0.32 episode 0.14\n",
      "episode  290000 win pct 0.30 episode 0.14\n",
      "episode  291000 win pct 0.29 episode 0.14\n",
      "episode  292000 win pct 0.34 episode 0.14\n",
      "episode  293000 win pct 0.24 episode 0.13\n",
      "episode  294000 win pct 0.34 episode 0.13\n",
      "episode  295000 win pct 0.32 episode 0.13\n",
      "episode  296000 win pct 0.38 episode 0.13\n",
      "episode  297000 win pct 0.30 episode 0.13\n",
      "episode  298000 win pct 0.36 episode 0.13\n",
      "episode  299000 win pct 0.42 episode 0.12\n",
      "episode  300000 win pct 0.30 episode 0.12\n",
      "episode  301000 win pct 0.24 episode 0.12\n",
      "episode  302000 win pct 0.41 episode 0.12\n",
      "episode  303000 win pct 0.30 episode 0.12\n",
      "episode  304000 win pct 0.38 episode 0.12\n",
      "episode  305000 win pct 0.33 episode 0.11\n",
      "episode  306000 win pct 0.34 episode 0.11\n",
      "episode  307000 win pct 0.41 episode 0.11\n",
      "episode  308000 win pct 0.45 episode 0.11\n",
      "episode  309000 win pct 0.47 episode 0.11\n",
      "episode  310000 win pct 0.37 episode 0.11\n",
      "episode  311000 win pct 0.40 episode 0.11\n",
      "episode  312000 win pct 0.33 episode 0.10\n",
      "episode  313000 win pct 0.32 episode 0.10\n",
      "episode  314000 win pct 0.51 episode 0.10\n",
      "episode  315000 win pct 0.38 episode 0.10\n",
      "episode  316000 win pct 0.45 episode 0.10\n",
      "episode  317000 win pct 0.45 episode 0.10\n",
      "episode  318000 win pct 0.41 episode 0.09\n",
      "episode  319000 win pct 0.40 episode 0.09\n",
      "episode  320000 win pct 0.35 episode 0.09\n",
      "episode  321000 win pct 0.33 episode 0.09\n",
      "episode  322000 win pct 0.41 episode 0.09\n",
      "episode  323000 win pct 0.42 episode 0.09\n",
      "episode  324000 win pct 0.40 episode 0.09\n",
      "episode  325000 win pct 0.49 episode 0.09\n",
      "episode  326000 win pct 0.44 episode 0.08\n",
      "episode  327000 win pct 0.61 episode 0.08\n",
      "episode  328000 win pct 0.42 episode 0.08\n",
      "episode  329000 win pct 0.53 episode 0.08\n",
      "episode  330000 win pct 0.50 episode 0.08\n",
      "episode  331000 win pct 0.44 episode 0.08\n",
      "episode  332000 win pct 0.42 episode 0.08\n",
      "episode  333000 win pct 0.58 episode 0.08\n",
      "episode  334000 win pct 0.51 episode 0.07\n",
      "episode  335000 win pct 0.47 episode 0.07\n",
      "episode  336000 win pct 0.52 episode 0.07\n",
      "episode  337000 win pct 0.54 episode 0.07\n",
      "episode  338000 win pct 0.53 episode 0.07\n",
      "episode  339000 win pct 0.49 episode 0.07\n",
      "episode  340000 win pct 0.52 episode 0.07\n",
      "episode  341000 win pct 0.54 episode 0.07\n",
      "episode  342000 win pct 0.45 episode 0.07\n",
      "episode  343000 win pct 0.50 episode 0.06\n",
      "episode  344000 win pct 0.43 episode 0.06\n",
      "episode  345000 win pct 0.43 episode 0.06\n",
      "episode  346000 win pct 0.57 episode 0.06\n",
      "episode  347000 win pct 0.54 episode 0.06\n",
      "episode  348000 win pct 0.69 episode 0.06\n",
      "episode  349000 win pct 0.53 episode 0.06\n",
      "episode  350000 win pct 0.61 episode 0.06\n",
      "episode  351000 win pct 0.58 episode 0.06\n",
      "episode  352000 win pct 0.47 episode 0.06\n",
      "episode  353000 win pct 0.58 episode 0.06\n",
      "episode  354000 win pct 0.59 episode 0.05\n",
      "episode  355000 win pct 0.61 episode 0.05\n",
      "episode  356000 win pct 0.50 episode 0.05\n",
      "episode  357000 win pct 0.60 episode 0.05\n",
      "episode  358000 win pct 0.54 episode 0.05\n",
      "episode  359000 win pct 0.57 episode 0.05\n",
      "episode  360000 win pct 0.48 episode 0.05\n",
      "episode  361000 win pct 0.52 episode 0.05\n",
      "episode  362000 win pct 0.51 episode 0.05\n",
      "episode  363000 win pct 0.48 episode 0.05\n",
      "episode  364000 win pct 0.55 episode 0.05\n",
      "episode  365000 win pct 0.59 episode 0.04\n",
      "episode  366000 win pct 0.61 episode 0.04\n",
      "episode  367000 win pct 0.61 episode 0.04\n",
      "episode  368000 win pct 0.46 episode 0.04\n",
      "episode  369000 win pct 0.57 episode 0.04\n",
      "episode  370000 win pct 0.61 episode 0.04\n",
      "episode  371000 win pct 0.63 episode 0.04\n",
      "episode  372000 win pct 0.53 episode 0.04\n",
      "episode  373000 win pct 0.52 episode 0.04\n",
      "episode  374000 win pct 0.50 episode 0.04\n",
      "episode  375000 win pct 0.56 episode 0.04\n",
      "episode  376000 win pct 0.59 episode 0.04\n",
      "episode  377000 win pct 0.56 episode 0.04\n",
      "episode  378000 win pct 0.55 episode 0.04\n",
      "episode  379000 win pct 0.58 episode 0.04\n",
      "episode  380000 win pct 0.60 episode 0.03\n",
      "episode  381000 win pct 0.60 episode 0.03\n",
      "episode  382000 win pct 0.63 episode 0.03\n",
      "episode  383000 win pct 0.54 episode 0.03\n",
      "episode  384000 win pct 0.59 episode 0.03\n",
      "episode  385000 win pct 0.65 episode 0.03\n",
      "episode  386000 win pct 0.69 episode 0.03\n",
      "episode  387000 win pct 0.63 episode 0.03\n",
      "episode  388000 win pct 0.63 episode 0.03\n",
      "episode  389000 win pct 0.65 episode 0.03\n",
      "episode  390000 win pct 0.62 episode 0.03\n",
      "episode  391000 win pct 0.62 episode 0.03\n",
      "episode  392000 win pct 0.68 episode 0.03\n",
      "episode  393000 win pct 0.75 episode 0.03\n",
      "episode  394000 win pct 0.58 episode 0.03\n",
      "episode  395000 win pct 0.56 episode 0.03\n",
      "episode  396000 win pct 0.61 episode 0.03\n",
      "episode  397000 win pct 0.58 episode 0.03\n",
      "episode  398000 win pct 0.66 episode 0.02\n",
      "episode  399000 win pct 0.62 episode 0.02\n",
      "episode  400000 win pct 0.67 episode 0.02\n",
      "episode  401000 win pct 0.56 episode 0.02\n",
      "episode  402000 win pct 0.60 episode 0.02\n",
      "episode  403000 win pct 0.62 episode 0.02\n",
      "episode  404000 win pct 0.64 episode 0.02\n",
      "episode  405000 win pct 0.64 episode 0.02\n",
      "episode  406000 win pct 0.57 episode 0.02\n",
      "episode  407000 win pct 0.55 episode 0.02\n",
      "episode  408000 win pct 0.67 episode 0.02\n",
      "episode  409000 win pct 0.55 episode 0.02\n",
      "episode  410000 win pct 0.62 episode 0.02\n",
      "episode  411000 win pct 0.64 episode 0.02\n",
      "episode  412000 win pct 0.72 episode 0.02\n",
      "episode  413000 win pct 0.62 episode 0.02\n",
      "episode  414000 win pct 0.67 episode 0.02\n",
      "episode  415000 win pct 0.65 episode 0.02\n",
      "episode  416000 win pct 0.73 episode 0.02\n",
      "episode  417000 win pct 0.62 episode 0.02\n",
      "episode  418000 win pct 0.60 episode 0.02\n",
      "episode  419000 win pct 0.56 episode 0.02\n",
      "episode  420000 win pct 0.72 episode 0.02\n",
      "episode  421000 win pct 0.65 episode 0.02\n",
      "episode  422000 win pct 0.64 episode 0.02\n",
      "episode  423000 win pct 0.66 episode 0.02\n",
      "episode  424000 win pct 0.57 episode 0.02\n",
      "episode  425000 win pct 0.68 episode 0.01\n",
      "episode  426000 win pct 0.64 episode 0.01\n",
      "episode  427000 win pct 0.68 episode 0.01\n",
      "episode  428000 win pct 0.68 episode 0.01\n",
      "episode  429000 win pct 0.63 episode 0.01\n",
      "episode  430000 win pct 0.72 episode 0.01\n",
      "episode  431000 win pct 0.77 episode 0.01\n",
      "episode  432000 win pct 0.67 episode 0.01\n",
      "episode  433000 win pct 0.71 episode 0.01\n",
      "episode  434000 win pct 0.70 episode 0.01\n",
      "episode  435000 win pct 0.66 episode 0.01\n",
      "episode  436000 win pct 0.73 episode 0.01\n",
      "episode  437000 win pct 0.73 episode 0.01\n",
      "episode  438000 win pct 0.65 episode 0.01\n",
      "episode  439000 win pct 0.71 episode 0.01\n",
      "episode  440000 win pct 0.60 episode 0.01\n",
      "episode  441000 win pct 0.66 episode 0.01\n",
      "episode  442000 win pct 0.71 episode 0.01\n",
      "episode  443000 win pct 0.71 episode 0.01\n",
      "episode  444000 win pct 0.60 episode 0.01\n",
      "episode  445000 win pct 0.77 episode 0.01\n",
      "episode  446000 win pct 0.71 episode 0.01\n",
      "episode  447000 win pct 0.72 episode 0.01\n",
      "episode  448000 win pct 0.70 episode 0.01\n",
      "episode  449000 win pct 0.69 episode 0.01\n",
      "episode  450000 win pct 0.63 episode 0.01\n",
      "episode  451000 win pct 0.70 episode 0.01\n",
      "episode  452000 win pct 0.72 episode 0.01\n",
      "episode  453000 win pct 0.68 episode 0.01\n",
      "episode  454000 win pct 0.65 episode 0.01\n",
      "episode  455000 win pct 0.71 episode 0.01\n",
      "episode  456000 win pct 0.62 episode 0.01\n",
      "episode  457000 win pct 0.68 episode 0.01\n",
      "episode  458000 win pct 0.63 episode 0.01\n",
      "episode  459000 win pct 0.66 episode 0.01\n",
      "episode  460000 win pct 0.73 episode 0.01\n",
      "episode  461000 win pct 0.69 episode 0.01\n",
      "episode  462000 win pct 0.66 episode 0.01\n",
      "episode  463000 win pct 0.71 episode 0.01\n",
      "episode  464000 win pct 0.71 episode 0.01\n",
      "episode  465000 win pct 0.70 episode 0.01\n",
      "episode  466000 win pct 0.71 episode 0.01\n",
      "episode  467000 win pct 0.54 episode 0.01\n",
      "episode  468000 win pct 0.63 episode 0.01\n",
      "episode  469000 win pct 0.74 episode 0.01\n",
      "episode  470000 win pct 0.64 episode 0.01\n",
      "episode  471000 win pct 0.69 episode 0.01\n",
      "episode  472000 win pct 0.68 episode 0.01\n",
      "episode  473000 win pct 0.67 episode 0.01\n",
      "episode  474000 win pct 0.57 episode 0.01\n",
      "episode  475000 win pct 0.67 episode 0.01\n",
      "episode  476000 win pct 0.62 episode 0.01\n",
      "episode  477000 win pct 0.71 episode 0.01\n",
      "episode  478000 win pct 0.70 episode 0.01\n",
      "episode  479000 win pct 0.63 episode 0.01\n",
      "episode  480000 win pct 0.72 episode 0.01\n",
      "episode  481000 win pct 0.75 episode 0.01\n",
      "episode  482000 win pct 0.68 episode 0.01\n",
      "episode  483000 win pct 0.63 episode 0.01\n",
      "episode  484000 win pct 0.74 episode 0.01\n",
      "episode  485000 win pct 0.65 episode 0.01\n",
      "episode  486000 win pct 0.75 episode 0.01\n",
      "episode  487000 win pct 0.58 episode 0.01\n",
      "episode  488000 win pct 0.74 episode 0.01\n",
      "episode  489000 win pct 0.71 episode 0.01\n",
      "episode  490000 win pct 0.71 episode 0.01\n",
      "episode  491000 win pct 0.67 episode 0.01\n",
      "episode  492000 win pct 0.68 episode 0.01\n",
      "episode  493000 win pct 0.67 episode 0.01\n",
      "episode  494000 win pct 0.64 episode 0.01\n",
      "episode  495000 win pct 0.64 episode 0.01\n",
      "episode  496000 win pct 0.65 episode 0.01\n",
      "episode  497000 win pct 0.68 episode 0.01\n",
      "episode  498000 win pct 0.63 episode 0.01\n",
      "episode  499000 win pct 0.71 episode 0.01\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl8VNXdP/DPd7JDFiAJW1jCEkRWQRYRRNxBKrauoD61VWtLi/L8tNpYrXsfUVuf6iPute4iLlUqIIosKgoSIEECAcISSAJJIAuEkP38/pg7k9lnkszMvTPzeb9eeXnvuScz34vJd07OPYsopUBEROHFpHcARETkf0zuRERhiMmdiCgMMbkTEYUhJnciojDE5E5EFIaY3ImIwhCTOxFRGGJyJyIKQ9F6vXFaWprKzMzU6+2JiELSli1bjiml0r3V0y25Z2ZmIicnR6+3JyIKSSJS5Es9dssQEYUhJnciojDE5E5EFIaY3ImIwhCTOxFRGGJyJyIKQ0zuRERhiMmdiKiDPsstQW1Ds95huMTkTkTUAT8V12Dhklz8+ZOf9A7FJSZ3IqIOONVobrEfPVGvcySuMbkTEXWG0jsA15jciYjCkE/JXURmishuESkUkWwX1weIyFoR2SYi20Xkcv+HSkRkQKJ3AK55Te4iEgVgMYBZAEYAmCciIxyqPQBgqVJqHIC5AF7wd6BEROQ7X1rukwAUKqX2K6UaASwBcKVDHQUgWTtOAVDqvxCJiHxX29CM/NIan+oerqxDafXpAEekD1+SewaAwzbnxVqZrYcB3CQixQBWALjD1QuJyO0ikiMiORUVFR0Il4jIs9vfysHs575DU0ur17rnPbUW5y5a07k3DPMHqvMAvKGU6gfgcgBvi4jTayulXlFKTVBKTUhP97qRCBFRu+UcrAIAtLQaNOsGiS/JvQRAf5vzflqZrVsBLAUApdQPAOIBpPkjQCIiX+QcrERm9nI0ai32b/ceQ2b2chyrbfDbe+yrqEVm9nLsKLHp9gnVB6oANgPIEpFBIhIL8wPTZQ51DgG4CABE5EyYkzv7XYgoaN778ZDd+T+/2w8A+KnEt/53X6zeWQbAvOyAlUH/QPCa3JVSzQAWAFgFYBfMo2LyReRREZmjVbsbwG9EJA/A+wB+pZQy6C0TkZFtKarE17vKnMpbWxVeWr8Px2obsHhtIV7/7gCO1rTNDhWHJrQlA63eWYath6o6FMu63eX48UAlAOBIzWm8u+mQ3WsDQO7hany10zlevfm0QbZSagXMD0ptyx60Od4JYKp/QyOiSHT1iz8AAA4umm1XvnpXGRatLMCilQXWsk+2FePzO84DAIhD94gl/7676RDe3XTI6fV88at/bbbG8ut/bcahyjqnOo0trfjNWzkdev1A4gxVIgoJ9c3Oo19qTjdZjytPNdpdO2FzDQAam1ud6tgqt1kjRilld15d14jqOvvX89bVvvVQFTbuP46mllYcrqxDdZ379w4En1ruRETBtm53OWac0dNjHUtXTF1jM9YUlNtdKzh60u58wXtb8eXOMpct7JU/HcH8d7fivd9MxrlD0rA05zD+9HHbao9nPfoV+qTE232Pp37nh5fl443vDzqVB7N1z5Y7ERnS9mL7B6GuHuNZumJONbR4fb0vPfSLb9aGT+4sPQEA2Li/0vm9bGPx8l5vbyzyGk+gMbkTUVB9vKUY93yY57WeJZl+tKUY936UhxfX7XOqU3S8Dvd8mOfU3+7JbW9uxpqCMrvhjK9vOAAAMIngrR8O4t/bHEd7A2LzJkoBT6/abXf954s34ER9EypPNbodY3/z6z/ig82HXF7zN3bLEFFQ3a0l9qevHeuxniWX/tHLB8GHW4qRPWu4z++/elc5Vu8qx8TM7k7XTAI8+Fm+T6+zpch+BE7u4Wqs2VXusV9//Z4KrN9TgesnDvA53o5icieioFBKYVX+Ua91gqX8pPPkJpPJ/Z8Atn8duJsYta+iFm/9oH+XDMDkTkRBsm5PBX73zlaPdT7e6twd4otth6rb/T1Fx52HNYqH/h3bS8vyXK+N+H9rCtsdR6Cwz52IgqKy1vtQwAoXrWlfnKhv8l7JB5667h0nSRkdW+5EZBi2reO/fbkHVXW+Je27lnp/QOuLBz7d4faaqwlMRsaWOxEZ1j+/O6B3CCGLyZ2IAm5fRS2e/Xqvy2uL1xZitzbhKLQ6PoyNyZ2IAu6m1za57NZoaVV4etVuXLn4Ox2iCm9M7kQUcA0u1oWxVd/UitYI2lwjGEM+mdyJSDe2SW7x2sJ2zTQNZR9sPuy9UicxuRORIeQVV4fccMOO2nXkRMDfg0MhiSggXvt2Pz7aUozRGSlup+SfqG8OclTG0BKEbhkmdyIKiMeX7wLgvPSuLdst8CJp77ZgPF5gciciXdz5/jZsL25bNuDrgnL07ZagY0TBE4wHqkzuRKQLV+uzGGEd9GBo9Tx4yC/4QJUogjW3tOJ0o/eNLrxRSqG2oRn1TS1oaglC5gpxrWy5E1Eg/fqNzfh277FOb//2xvcH8ch/dgIAhvdOwhf/Pd0f4YWtYPS5s+VOFMG+3XvML6/zxY62ddo9PUAls+nD0gL+HkzuRERBlpnaNeDvweRORBRkpiBMxWWfOxG1i1IK3+49hozuCYgxmTAgtQscu5ALjgZ+BmYoC8YyC0zuRNQun2wtsW5yDcDlw9iZ//g2mCGRC+yWIaJ2Ka0+7VwYQbNLQwWTOxH57F8bDuDvX+1xKg/GuG1qHyZ3IvLZP1a73k2pOYLWYg8VTO5E5JZSCs+v2YuKkw0AXLfQtxRVIfdwtVM56YvJnYjcyj1cjb99uQd3Lc0F4Hrlxqtf/D7IUZEvmNyJyK3DVeaHp3Xa+jPsWw8dTO5E5FJ9UwvufH+bXRmTu38E45+RyZ2IXHK1qXU4PDeNNkXGVn5M7kSEhz7bYXd+5/vbcNcHuXZln+WWoNFFwg8152UFftEub6KC8AHD5E5EePMH+00yluWV4uuCcruyhUvsk32omj9jqN4h4Mw+SQF/DyZ3ogj0zZ4K1zNN3Sg4Ej5rxcTHtD/tDe2Z2KH3ynCzbaBw4TAiCoRfvv4jkuN9//U/5YfdmoxiYAeW2/Xng+QJA7v77bU88ekjTERmishuESkUkWw3da4TkZ0iki8i7/k3TCJypJRCi8MTTsdzT07UNzu9XmurCutt8qYOTUVKQoxdWf8eCdh430U+fX9aYqz12Je+e8cG+sFFs/HR/HN9eq/O8prcRSQKwGIAswCMADBPREY41MkCcB+AqUqpkQD+OwCxEpGN59cUYsifV+BUgzlJL805jCF/XoGSdnS32Jr93HcY/OcVyLp/pT/DNLz46CjvS/BaPzPbKvrSmA/G0r7u+NJynwSgUCm1XynVCGAJgCsd6vwGwGKlVBUAKKXKQUQB9d6PhwAANaebAADLcksBAPsrajv0ejvDqF99eG/XDywtCXnbXy7BuUNSAQDdu8bC19xum6yVwZfC9CW5ZwA4bHNerJXZGgZgmIhsEJGNIjLTXwESEbXXEC8PQLt3jcUdF2YB0NriXrK70j4VxK7Mexzi9WMjcPw1WiYaQBaAGQDmAXhVRLo5VhKR20UkR0RyKioq/PTWRJHN2O1HfTww+0yX5U9cNdp6bGl5izhve3fO4B5e32PRVWOcys4flm53bvRumRIA/W3O+2lltooBLFNKNSmlDgDYA3Oyt6OUekUpNUEpNSE9Pd3xMhEFUGNzK7YXVyO/tEbvUAKuT4rrIYjuRso45uCLz+xld255Tm37ITAgtQu6dTE/nE2IiQIA/PUXozoQbWD4ktw3A8gSkUEiEgtgLoBlDnU+hbnVDhFJg7mbZr8f4yQiN3xtHP51+U7MeX4DZj/3XUDjCYTB6e0fvuiVzZ88juPOHVvyyQnmYaNXjbfvkbbUctf/rudCB16Tu1KqGcACAKsA7AKwVCmVLyKPisgcrdoqAMdFZCeAtQDuUUodD1TQRNTG126ZvOLQbLGvXHge/rNgmvX8hRvH49qz+3X6da0PSV2k4Jgo+7KusdHY9ehM/PHSM+zKLR8Klpa944dEMCYruePTLAal1AoAKxzKHrQ5VgDu0r6IKAi8pY0739+G46ca8O5t5wBAyG6oERttsluLJT7G5FNfdmJcNGobmr3Wc/VacVo3i62EWOeyjG4JqDzVCEt4ji9l6JY7ERmTtxb7srxSbCgMjz+g422Sra+TRVcuPM+61MA1Z/eza/17e524aBMW3zAe/7j+LI/v8fqvJuLZuWehW0Lb5KZXfzmhrYLBH6gSUQgw+rjrzrpweE+f6lnGr/fv0QWXjewNAJg2NA2j+6W4rC/iOgfPHtMHPZPjPL5XelIcrjwrw27kzSUj2h7GsuVORO0WGauSO9+nUsCvzh3ktv7Ci9oG6v3mvMFIjIvG1KHOSwXYfhi66+Zx1R9/XlYa7rzIfjDgI3NGoXdyPFK7mj8Mzh+WjgUXtK0+GRtlwmNXjnQbcyBw4TCiEOWunV55qtHuvLahGYlxof+rbptmR/RNRlpiLI7VNmLz/Rdj4l9XAwA+v2MaRmW0tdBHZaRgxyOXuXw9S7dMeycavX3rZKeymaN6Y+ao3tbzN2+ZBAD4Iv8oAGD5ndOQ1Svwy/zaYsudKMwsXJKLDYXHrOejHlqlYzS+iYv2norGDTDPi+zTLR4ArK1x2yV8OzI4JZADWiyToRwXKwsGJneiEOUpJ+UVh87ImPdum4xJg7zPCP39jKH46v9Nx8i+5pb5U9eMwdo/zkBSfOASZ2efYzx0xUh8fff56Jkc76eIfMfkThQBnli5S+8Q3Dp3aJrHlrtlrLjJJHZdG3HRURiUZj+5yXHykSeWurFRpoCtARMTZcKQ9I5t9NFZTO5EEeDl9fpPGJ8+zP2SI09cNQa/O39Ih1/71mmDcOHwnm5Xg3RlypBU/O78IXji6tFI6RKDey47AyP7Jnc4BqMJ/acsRBHo611laPSwqUa9AXdOev3mCYiOMmHkg1847eyUnhSH7FnDsW53OQqOnmz3a//lZyO8V3IQZRJkzxpuPf/DBUNRcPQk8kudlz7WcwGwjmLLnSjEbCmqxK1v5uBYbaPbOs+tKQxiRL6xdK/Mn9HxFnqg/WJcXwDAWf3ND28te6fOnThAt5g6ii13ohBTdarJ7lz5cX/PYFhwYRYWXJiFzOzleofi5MLhvXBw0Wzrec+keLvzUMLkThRicoqq7M5vf2sLdh45gR5dY918R2gLwR4RQ2C3DFGI+XLnUbtzy/Z4jpOXgq1blxjrmHPHB5sv3XS23eJfFHhM7kShxqC9MNEmE3473dyfnp5kvybLFG29F29c9TCF4sNMI2ByJyK/WHzDOOvxyL4pSNW6iaYPS0dSB5c/GDegGzK6ud5ViTxjnztRiFBKYU9Zrd5huPTOrZMxeXAqNu6vBADERgm2/OWSTr/uG7+ahOgotkE7gsmdKER8tKUY93y0Xe8wPLKs/3KW9l/SD5M7UYiwPDg1Iku/+PRh6ch54GKkJXpeB50Cj3/vEIUIIw9nt33m2ZnE3q1L8FdPDFdM7kQhItQmK3XE8zeMx6NXjkRSPDsVOovJnShEGDq1+2m4YnpSHH45JZMTl/yAyZ0oRLQauOXu7yVzH5g9AlEmQZe4KO+VySX+7UMUIgyc2/3uuon9cd3E/nqHEdLYcicKEUbO7ZxFajxM7kQhQs8HqudlpXm8ztxuPEzuRAZ2tKYeox9ehczs5TjuYf32QBuVkeLxurDpbjhM7kQG9tcVu3CyvhkA8OXOMt3i8LbBRkdy+x8vHYaP50/pYETkDZM7UYC1tip8v++Yz3U3FB6zdsFUnmoIZGg+S453nlw0b1LnHnguuDALZw/s0anXIPeY3IkC7NVv9+OGVzdhbUG517rvbCrCja9twsod5jXbNxQeD3R4fjGwRxe9QyAHTO5EAXbg2CkAwNET9V7rllSdBgAUHa8LaEwdsf9/LseBJy53Kn/s56PQMzleh4jIEyZ3ogDzNMjlwc92YNyjXwIAXl6/Dy9/sx8AsKWoynB7jJpMYvfgND3JnNC7cz0YQ+IkJiIdvfVDkfX4iZUF1uPVu/R7eAoAUSZBS6v5U+mpa8a4rHPHhUORmdoFs0f3CWZo5CO23IkMYEOhbw9cA+m/zhnosvzM3skuy2OiTLhqfD8OgzQoJneiIPGUAm98bVPQ4nDn11MzAQBJcdF249qVoefGkjvsliEKMFfJsbVV4YgPD1iDaXB6Ig4umg0AmPfKRp2joc5iy50oSGx7L15cvw9TF63RL5hOmJTJsemhgC13Ih0YoY/dYlivRHz4u3PdXncc7fPWrZNw4nRTgKOizmJyJwowoy/V2y0hFikJ9sMZu8a1pYYok/3TgviYKMTHcJ11o2NyJ9KB0QeYPHn1aPxjdTx6JcdhZF/Xo2XI2HzqcxeRmSKyW0QKRSTbQ72rRUSJyAT/hUgUfvy9c5EvZo9xMx7dRSipiXF47OejsODCLA51DFFek7uIRAFYDGAWgBEA5onICBf1kgAsBKD/mC4iA9Ijodu6fgJ3NookvrTcJwEoVErtV0o1AlgC4EoX9R4D8CQAY43vItKZUbrcpw9Lx3PzxlnP75s1XMdoKNB8Se4ZAA7bnBdrZVYiMh5Af6WUsRbDIAqwozX1yMxejszs5Zj/zhYAQGb2ctz5/jYAwI6SGny0pRgAcO/H27GvohYAsPVQlS7xpifGWY+N8qFDgdHpce4iYgLwDIC7fah7u4jkiEhORUVFZ9+aSHeWZA3AukwvACzLKwUA/Hig0q7+jpIaAEBdY0sQonM2ZUiq9djoo3ioc3xJ7iUAbDvr+mllFkkARgFYJyIHAZwDYJmrh6pKqVeUUhOUUhPS09M7HjWRQS3NOezx+sIluXhhXWGQonHtrP7dAACtWnbn49Lw5Ety3wwgS0QGiUgsgLkAllkuKqVqlFJpSqlMpVQmgI0A5iilcgISMZGBOLZ+7/1ou9fveeqL3QGKxjcmZvOI4DW5K6WaASwAsArALgBLlVL5IvKoiMwJdIBEocyIPR8mbWijZUlfCk8+TWJSSq0AsMKh7EE3dWd0PiwiCpSZo3ojp6gKGd0S9A6FAogzVIkizK3TBmHupAHYXlytdygUQFwVkqiDvsw/ipv+6X7O3tOrCtxe05OIIDEuGgna+jBpNsMjKXwwuRN10DNf7fF4ffHafUGKxLOx/VKw6c8XOZWf1b8bnrhqNJ64erQOUVGgsVuGqJ1+Kq7BsVMNPtV9fPnOAEfj2hm9krC77CQA4LMF01zWERHMmzQgmGFRELHlTtROVzz/HX79r80+1Q32RKGMbglIjo/GvTPPCO4bk+Gw5U4UJqYOTcW7t52jdxhkEGy5E4WJsf266R0CGQiTO1EIGpTW1ansrkuG6RAJGRWTO5GmsbkVt7yxGfmlNXblx2obcMOrG3G81reHqMHgKrlHR/HXmdrwp4FIs/voSawpKHdaH+atH4rw/b7jeGfjIZ0ic6YcntS+e9tknSIho2JyJ9JYdpNTyrzuyte7ynC0pt66TG+LUli9s8xav+DoST3CdGnq0DS9QyCD4WgZIhde/maf0+qNH+UcxnNf79UpIntc8ou8YcudyIECUFx12qm8tIY7SFLoYHIn0rR1yxi/Xdy/exe9QyCDY7cMkUZs9iQy6n4WL9w4HiP7JqNXcjymZaXht29vcVt35cLzEM2dOSIWkzuRg4KjJ5GSEKN3GE5evHE8Zo3uYz2/bGRvj/XP7JMc6JDIwJjciTRi08jd5LCxtRGIi0b4n2YOx8n6puAHQ4bH5E4Rr7VV4eDxUy6Tp7E4Bzh/xhAd4qBQwAeqFPFeXL8PF/59PXYbaNy6K+w+p/ZgcqeIl3PQ3AVTUu08/NEILj6zJwDz+utEvmJyp4iztqAc055cg4bmFrtyMegYmaR488Pd2Gj+upLv+NNCEeehZfkorjqNI9X2k5KM1DD+7A9TrccPzxmJ+2YNx/QsLjFAvmNyp4jV0NwKAMgvPaFzJGbxMeZfx2vO7oex/dvWZk9JiMFvzx/CbhlqFyZ3ijiHKusAAE+vMq8dU37SOEv5EvkLkztFhJZWhaaWVruyitoG1De12NXRk1H7/Ck0MblTRPj1G5uRdf9KbDtUZS3LO1yN4X/5wnpuacnrZUAPrhdD/sPkThHhmz0VAIAtRVVeaupn9hjz0gJsv5M/MLmTISil8Mh/8p22uHPnVEMz7vogF9V1jW7rvPn9Qaz46Yhd2ePLd3UqTqJQweROhlBd14R/bTiIG1/b5FP9dzcV4ZNtJXh+TaHbOg8ty8fv393qrxCD7u1bJ+G+WcP1DoNCFJM7GYqvS6lb6pm0OfkHjp3C4co6bD5ovAW/XPn23gucyhzv/bysdPz2fK4dQx3DhcMoJFnyoKV/+oK/rbNe++wPU+3Gia8paNv3lChSsOVOIUk5Zncbx0/Zj1svqTbW9nh7Hp9lPe6VHIdeyXF21zlXifyBLXcyhPYmtFYtu7+8fj+6JcTaXVMKyMxe3vbanY7Ov2KjTdZ1YjJTu6K5VaHsRANSE833kZoY5+nbiXzC5E4h78kvCvQOwWdf/b/pAIBeyfF44cbxmDI4FQrApv3HMXNUb8RFmzDnrL76BklhgcmdDMXXzanbs4n1A5/u6Gg4fpfVK8l6fLnNlnmW7fOundA/6DFReGKfOxlCe6fe67xSAJHhMblTyDlW24DG5la319vRqCcKW+yWoZAz4fHVHq8fqTHmjkpEwcSWO4WdshPGXML34/lT9A6BIohPLXcRmQngWQBRAF5TSi1yuH4XgNsANAOoAHCLUqrIz7FSBLvz/W0YlZGMf28r9Vq3wKAbXffoyiGOFDxeW+4iEgVgMYBZAEYAmCciIxyqbQMwQSk1BsBHAJ7yd6AUGdx1ly/LK8X/rCjAriPed01avUufGak/dxjC+Pdrx1qP77nsDGSmcklfCh5fumUmAShUSu1XSjUCWALgStsKSqm1Sqk67XQjgH7+DZPCnjZY5mR9s9Ol5hb3D0+N5B9zxyGrZ6L1/Oqz234N/nDBUG6TR0HlS3LPAHDY5rxYK3PnVgArXV0QkdtFJEdEcioqKnyPkiLaP787oHcIPvvNeYP1DoEIgJ8fqIrITQAmAHja1XWl1CtKqQlKqQnp6en+fGsKY0Z9QOrKdRM5CYmMwZfkXgLA9ie2n1ZmR0QuBnA/gDlKqdD5bSRjsOlst10XJtQlx3O0MenDl5+8zQCyRGQQzEl9LoAbbCuIyDgALwOYqZQq93uUFNGU28esxrf67vNxtMZYq1JSZPCa3JVSzSKyAMAqmIdCvq6UyheRRwHkKKWWwdwNkwjgQ+2h0SGl1JwAxk1h5EjNaTzz5R67suXbj+AP74XuLkoWPZPi0TMpXu8wKAL59DejUmoFgBUOZQ/aHF/s57gogty9NA/f7ztuVxYqif2/zhmI1MRY9E1J0DsUIjvsEKSAq21oRvmJegxOT3R5vcHDOjFGNn/GEPxpJvc4JWPi8gMUcDe8uhEX/n292+utIbrS16TMHnqHQOQWW+4UcNuLazxeN+ryvT26xqLyVKP1fPvDlyLGZILJBNQ3tSIlIUbH6Ig8Y8udfJZfWoPM7OUoqfZ91cWnbHZJOlbbgMzs5cjMXm43gqTVoNm9f3f7fvTk+BgkxEYhLjqKiZ0Mj8mdfPbepkMAgDXtWLvlhXX7rMc/Hqi0Hn+zt22Gst7dMgNdrPkyd2J/PDxnZIde793bJuPTP0ztbFhEncJuGXJrTUEZPs87gr9dOxYmk1g3sXZMxWsKytC3WwKG9062K1/y4yG7c9sFvd78/iCiRJBXXO3TYmCB9LMxfbB47T67sl9OyURMVMfWgpk6NM0fYRF1CpM7uXXLGzkAgItH9MLlo/u43QrPUu/gotnWsn0Vtcj+5Ce7ep9sbZvYnF96And/mOfvkDvkF+MynJK7yQRwnS8KZeyWIa9Kq0/jRH0TmtvRN14WIrMy503qj6E9k5zK27unK5HRsOVOXj2+fBceX77Leu6ti/xwZR1ueG1TgKNqPxFXsbtO4my1U6hjy53aTXnJ7kXH6zxe14vJRcZ2l8QF3GibQhtb7mSnsbkVd7y/1e1sUsD8QPWFdYXolhCLGyYPsJYbfTVHkwAtDmXuGuiuW/lEoYPJnexsL67GqvwyAJ6HOz71xW4AsEvuRmde1M6csYf1SsSeslpPtSHSlt0/v2NaYIMj8jN2yxAA4FRDMzbuP+7TBCXbFm1huTE3o3bFtpV+0zkDzWUOTfdBaV0BmFv5FkPSu2JURkqAoyPyLyZ3AmBemXHuKxuxcEmu17q2vRUXP/NN4ILys9unt22BZ/mAsoyKuVbb7/RG7S+R9KQ4pGtL9d4weWAQoyTyD3bLEACg4Ki+E4mC4e5Lz8D/rSm0K7O03J++diyevnYsAOA2m31QbcfuE4USttwj3Nsbi5CZvRx1jY6PGt177POdAYwoOHp0jQUA9ErmRhoUnthyj3AvaWu/HLdZ/TCcrfvjDFTVNeKs/t0AALNG9dY5IqLAYHKPMDV1TVi8rhAl1acx1Ga4Y4tBV2b0t8y0rsiE+aHpFWP76hwNUeAwuUeYR/6Tj0+2ta3xEhsdPj1zvzt/CF5av8+p/E8zh6P8ZGgsh0DkL+Hzm21AB46d8jqbEzCvZ150/JTPr1tcVYdGbWu6usZmlJ2wT1xVpxpRXdeIw5V12Ft2Eit+OoKcg5U4Wd+EAw7v02jwLe5GZSR7rwTgzD7JuGREL5fX5s8Ygoeu6NjyvUShii33ANm4/zjmvrIRT149GtdP9DzRZ/HaQvz9qz1Yfdd0l4tY2aprbMa0J9fiqnEZeOb6s3Ddyz9gR8kJu1Ed4x77yi/3YARTBqdiR4n3kTwXnJGO3in2D0cvGdELX+30fe15onDC5B4gheXm2Y+5h2tw/UTPdTdpm1iUVtd7Te6ntVEt6/aYN7vwJfGFstvOG4xXvz3g8tqex2fhWG0DokyCtMQ4RJkEG++7CLUNzUjtGosucVE4Wd8c5IiJjIHJPUDaFqmy75apa2zGrGe/xd+uHYuJ2gbL7jbB8KSQlwcBAAAMpUlEQVTyVKPdWi5//DAPvZLj0NBk7G4Wf4qNNqFvN/ut8Bxb73GJUcEMicgwmNwDxJKwWx1y7c7SEyg6XodFKwvw8fxz7a750j/vzkdbijv8vUb3/A3j8O7GQ9hbXotjtQ0Y2jPR+hcMEbkWsQ9Uaxua8VluifeKMA8f/E9eqcc6n28vRXVd21jxipMNAACltcf3lJ3EwiXb8NSq3U7fW6qt5/LOxkN4af0+bC+u9imuUHXxma4ffLqiFPCzMX3x/u3nWD8w37ttMjZkXxig6IjCQ8S23B/490/4NLcUmaldMVab0OLOwg+2Yd3uCozpl4KBqV2drpdUn8aC97Zh2tA0vHPbZADAM1/tAQBYho9f+r/2a7DYrle1r8I8gmX1rjLrPqPupr1/V3jM670ZncmHjTDOH5aO9Xsq0K1LTOADIgpDEZXcm1parX3hZSfMLevq0012dZRSaGlV2vKwQJRJUFJlblnXu+nPrm8ydxEUVzlvUtHc0oqmFufvO9XYgvqmFkS5yXT1TS0QMS9sFWUSKGX+G6C6rsll/VDiatMMR9dN6I83b5kUhGiIwlPEJPfKU40Yrw0RTO0ai+F9zKNSbn79RxQ8NhPxMeYHb3/6eDuW5hQjOT4a8TFR+PH+i72+tmWtFVeTPD/NLcWnuc5dOruOnMDwv3zh9jVtr0WbpF37lxrdsN5J+CL/qMc6PZPjnMrG9kvB6l3liIvmQ1IibyImuR+12bD5+KlGu9ZjfVOLNbkvzTE/mDxR34wTDsPo3DU41+02D0tsdtFC94dQTuxpibE4VtuI7l1iUKX91bHwoizkHa7Gem04p6MPbj/HOpLI1rNzx2FveS1S2FVD5FXIPlB9fs1ebNx/3Gu9D3MO47PcEo8bHntKnvd8mGcdK33p/36DzOzlqDrViMn/sxqZ2ctx99I8a92mVgWllOG3mwumB2aPAAC7zS6iTIKJmd3dfs/kwakuy7vGRVsX/CIiz0I2uf/tyz2Y+8pGr/Xu+Wi7yw0oiqvadhxaW1Du9vs/3FKMow7T++94f5u1z/7jrW1DEJtaWnGygZNmLG6cPADdtaV1AeDj+VMwf8YQAMDN52Zayx+Yfab1+MGfjQhafEThLGSTu0Vh+UkopbD5YCVaWhV+8cIGFJbXYktRlV03Sd5h++GFB461rbGSX3oCy/JKkXvYtyGI7kasVNc1YbM22zRS/dZmt6OH54y0GxV09sAe+NPM4QCApPi2rhXbzTFumTYo4DESRYKQ73O/+Jlv8NjPR+Evn+6wKVsPwLxKoEX2Jz+5fY03vj+IN74/6Jd4bn0zxy+vEwquGpdht8IkAEwdmoaXv9kPwDzc09LhJS76xS4b2UvbjJuI/C3kkzsA7Ct3vYt9JGwdp5fdj89EjMmEJ64ejTMeMI/s2fXoTCTERkHEPPlIRKyzbl098njhxrMjZh15omAL+W4ZAG5b3ZZRLOR/cdFRMJkEcdFR6KOt55IQax5xNKBHF59eI8okYbWePJGRhEXLnQLvxRvHY0tRFV77znmFxk9+fy62F9dYz5f+dgq2HaoyT77SyrzNW/r8jmnWyWBE1Hkh12xyXA2R/OPas/u5vXbdhH6YNboPZo3uAwAYN8B+OGKflARcNrJtL9JeyfGYOaqPXR1vc1JHZaRggoux7UTUMSGX3N/+oUjvEMLS72YMQXqSeVbowFT7bhXLCJbhvZPQKzkO91423OfXHT+gO9IS43DHRVn+C5aIvPKpW0ZEZgJ4FkAUgNeUUoscrscBeAvA2QCOA7heKXXQv6GanagP/bVVgmnyoB7WzUBsHVw0G5/llmDhklzMGdsXQ9ITsdlmqYXPt5diwXvbMHt0Hwzvbd7qrmtcNDb92ftyDLZSEmKQ80D7voeIOs9ry11EogAsBjALwAgA80TEcabJrQCqlFJDAfwvgCf9HSh1zAQPM0EtXI1XsTwUHds/xcVVIjI6X1rukwAUKqX2A4CILAFwJYCdNnWuBPCwdvwRgOdFRFRndp8IYWcP7I6rxmdgdEYK5jy/ocOv89TVY6Cg0CclAQNTu2B7cQ12lNRYx5Ffc3Y/6yYdW/9yCfaWnUSX2Gh072qeIFRzugln9ErCrFF9cKSmHl1ioyAAhvRMtHsfV/+bxvTrhtV3nY8h6c5LHBOR8fmS3DMAHLY5LwYw2V0dpVSziNQASAXg98XHQ2FcdFbPRNw4eaBd2QVnpGNtO4dmXjexv935wNSuuGJsX2tynz4s3Zrce3SNdVqTpZ/WaB+VkWK3totFbJT5Dzd3wxGHOnwIEFHoCOpQSBG5HcDtADBgwIAOvcaUIakeZ5POHt0HO0prUHTceW11AHjoihF45D/mPzquGp+BT7a2zbC0JODhvZOQHB+DHw+29VWnJMTg9zOGIC0xDo8v34kLhvfENWf3w8qfjuLtjUV46abx2FtWi9rGZvx+xlDr9/3q3Ewcq23AQ1eMxBvfH0CUyYQh6V3R0NyK4qrTmDI4FdsOV6GlRaFnchxio01YlluKkX3dd4e89ssJaG5VuPjMnng0MRaLrhrj6z+fnUtG9ML8GUPslgwgovAg3npORGQKgIeVUpdp5/cBgFLqCZs6q7Q6P4hINICjANI9dctMmDBB5eREzlR9IiJ/EJEtSqkJ3ur5MhRyM4AsERkkIrEA5gJY5lBnGYCbteNrAKyJ1P52IiIj8Noto/WhLwCwCuahkK8rpfJF5FEAOUqpZQD+CeBtESkEUAnzBwAREenEpz53pdQKACscyh60Oa4HcK1/QyMioo4KuRmqRETkHZM7EVEYYnInIgpDTO5ERGGIyZ2IKAx5ncQUsDcWqQDQ0fV70xCApQ0MjvccGXjPkaEz9zxQKZXurZJuyb0zRCTHlxla4YT3HBl4z5EhGPfMbhkiojDE5E5EFIZCNbm/oncAOuA9Rwbec2QI+D2HZJ87ERF5FqotdyIi8iDkkruIzBSR3SJSKCLZesfTGSLyuoiUi8gOm7IeIvKViOzV/ttdKxcReU677+0iMt7me27W6u8VkZtdvZcRiEh/EVkrIjtFJF9EFmrl4XzP8SLyo4jkaff8iFY+SEQ2aff2gbacNkQkTjsv1K5n2rzWfVr5bhG5TJ878p2IRInINhH5XDsP63sWkYMi8pOI5IpIjlam38+2UipkvmBecngfgMEAYgHkARihd1yduJ/pAMYD2GFT9hSAbO04G8CT2vHlAFYCEADnANiklfcAsF/7b3ftuLve9+bmfvsAGK8dJwHYA/Om6+F8zwIgUTuOAbBJu5elAOZq5S8BmK8d/x7AS9rxXAAfaMcjtJ/3OACDtN+DKL3vz8u93wXgPQCfa+dhfc8ADgJIcyjT7Wdb93+Qdv7jTQGwyub8PgD36R1XJ+8p0yG57wbQRzvuA2C3dvwygHmO9QDMA/CyTbldPSN/AfgMwCWRcs8AugDYCvMexMcARGvl1p9rmPdNmKIdR2v1xPFn3baeEb8A9APwNYALAXyu3UO437Or5K7bz3aodcu42qw7Q6dYAqWXUuqIdnwUQC/t2N29h+S/ifan9ziYW7Jhfc9a90QugHIAX8HcAq1WSjVrVWzjt9tsHoBls/mQumcA/wBwL4BW7TwV4X/PCsCXIrJF2y8a0PFnO6gbZFP7KKWUiITdcCYRSQTwMYD/VkqdEBHrtXC8Z6VUC4CzRKQbgH8DGK5zSAElIj8DUK6U2iIiM/SOJ4imKaVKRKQngK9EpMD2YrB/tkOt5V4CoL/NeT+tLJyUiUgfAND+W66Vu7v3kPo3EZEYmBP7u0qpT7TisL5nC6VUNYC1MHdJdBPzZvKAffzWe9OupwA4jtC656kA5ojIQQBLYO6aeRbhfc9QSpVo/y2H+UN8EnT82Q615O7LZt2hznaz8Zth7pe2lP9Se8p+DoAa7c+9VQAuFZHu2pP4S7UywxFzE/2fAHYppZ6xuRTO95yutdghIgkwP2PYBXOSv0ar5njPrjabXwZgrjayZBCALAA/Bucu2kcpdZ9Sqp9SKhPm39E1SqkbEcb3LCJdRSTJcgzzz+QO6PmzrfdDiA48tLgc5lEW+wDcr3c8nbyX9wEcAdAEc9/arTD3NX4NYC+A1QB6aHUFwGLtvn8CMMHmdW4BUKh9/Vrv+/Jwv9Ng7pfcDiBX+7o8zO95DIBt2j3vAPCgVj4Y5kRVCOBDAHFaebx2XqhdH2zzWvdr/xa7AczS+958vP8ZaBstE7b3rN1bnvaVb8lNev5sc4YqEVEYCrVuGSIi8gGTOxFRGGJyJyIKQ0zuRERhiMmdiCgMMbkTEYUhJnciojDE5E5EFIb+P6qq66iJimMOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(n_games):\n",
    "    done = False\n",
    "    observation = env.reset() # state\n",
    "    score = 0.\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.choose_action(observation)\n",
    "        next_observation, reward, done, info = env.step(action)\n",
    "        # update agent q-value estimates\n",
    "        agent.learn(observation, action, reward, next_observation)\n",
    "        score += reward \n",
    "        observation = next_observation\n",
    "    scores.append(score)\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        win_pct = np.mean(scores[-100:]) # past 100 episodes\n",
    "        win_pct_list.append(win_pct)\n",
    "        if i % 1000 == 0:\n",
    "            print('episode ', i, 'win pct %.2f' % win_pct, 'episode %.2f' % agent.epsilon)\n",
    "plt.plot(win_pct_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see an overall upward trend in the agents win percentage  over the course of the games and it tops out around 70% average with some win rates over 80%.\n",
    "\n",
    "We are able to achieve a consistent 70% win rate which is a huge improvement over our previous 20% rate from earlier approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frozenlake soved naively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "PERCENTILE = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteOneHotWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(DiscreteOneHotWrapper, self).__init__(env)\n",
    "        assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
    "        self.observation_space = gym.spaces.Box(0.0, 1.0, \n",
    "                                                (env.observation_space.n, ), \n",
    "                                                dtype=np.float32)\n",
    "        \n",
    "    def observation(self, observation):\n",
    "        res = np.copy(self.observation_space.low)\n",
    "        res[observation] = 1.0\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = DiscreteOneHotWrapper(gym.make(\"FrozenLake-v0\"))\n",
    "env = gym.wrappers.Monitor(env, directory=\"mon\", force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_size  = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "objective = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params=net.parameters(), lr=0.01)\n",
    "writer = SummaryWriter(comment=\"-frozenlake-naive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
    "    \n",
    "    obs_v, acts_v, reward_b, reward_m = filter_batch(batch, PERCENTILE)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    action_scores_v = net(obs_v)\n",
    "    \n",
    "    loss_v = objective(action_scores_v, acts_v)\n",
    "    loss_v.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    print(\"%d: loss=%.3f, reward_mean=%.1f, reward_bound=%.1f\" % (\n",
    "        iter_no, loss_v.item(), reward_m, reward_b))\n",
    "\n",
    "    writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
    "    writer.add_scalar(\"reward_bound\", reward_b, iter_no)\n",
    "    writer.add_scalar(\"reward_mean\", reward_m, iter_no)\n",
    "    \n",
    "    if reward_m > 0.8:\n",
    "        print(\"Solved!\")\n",
    "        break\n",
    "            \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we incorporate the calculation of discounted reward and return \"elite\" episodes, in the training loop, we will store previous \"elite\" episodes to pass them to the preceding function on the next traiing iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_batch_discount(batch, eprcentile):\n",
    "    disc_rewards = list(map(lambda s: s.reward * (GAMMA**len(s.steps)), batch))\n",
    "\n",
    "    reward_bound = np.percentile(disc_rewards, percentile)\n",
    "    \n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    elite_batch = []\n",
    "    \n",
    "    for example, discounted_reward in zip(batch, disc_rewards):\n",
    "        if discounted_reward > reward_bound:\n",
    "            train_obs.extend(map(lambda step: step.observation, example.steps))\n",
    "            train_act.extend(map(lambda step: step.action, example.steps))\n",
    "            elite_batch.append(example)\n",
    "            \n",
    "    return elit_batch, train_obs, train_act, reward_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.envs.toy_text.frozen_lake.FrozenLakeEnv(is_slippery=False)\n",
    "env = gym.wrappers.TimeLimit(env, max_episode_steps=100)\n",
    "env = DiscreteOneHotWrapper(env)\n",
    "\n",
    "env = DiscreteOneHotWrapper(gym.make(\"FrozenLake-v0\"))\n",
    "env = gym.wrappers.Monitor(env, directory=\"mon\", force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "objective = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params=net.parameters(), lr=0.001)\n",
    "writer = SummaryWriter(comment=\"-frozenlake-tweaked\")\n",
    "#writer = SummaryWriter(comment=\"-frozenlake-nonslippery\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_batch = []\n",
    "for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
    "    reward_mean = float(np.mean(list(map(lambda s: s.reward, batch))))\n",
    "    full_batch, obs, acts, reward_bound = filter_batch(full_batch + batch, PERCENTILE)\n",
    "    \n",
    "    if not full_batch:\n",
    "        continue\n",
    "\n",
    "    obs_v = torch.FloatTensor(obs)\n",
    "    acts_v = torch.LongTensor(acts)\n",
    "    full_batch = full_batch[-500:] # last 500 episodes\n",
    "    \n",
    "    optimier.zero_grad()\n",
    "\n",
    "    action_scores_v = net(obs_v)\n",
    "\n",
    "    loss_v = objective(action_scores_v, acts_v)\n",
    "    loss_v.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(\"%d: loss=%.3f, reward_mean=%.3f, reward_bound=%.3f, batch=%d\" % (\n",
    "            iter_no, loss_v.item(), reward_mean, reward_bound, len(full_batch)))\n",
    "    \n",
    "    writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
    "    writer.add_scalar(\"reward_mean\", reward_mean, iter_no)\n",
    "    writer.add_scalar(\"reward_bound\", reward_bound, iter_no)\n",
    "    \n",
    "    if reward_mean > 0.8:\n",
    "            print(\"Solved!\")\n",
    "            break\n",
    "            \n",
    "    writer.close()\n",
    "    \n",
    "env.close()\n",
    "env.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To monitor and check the agent in action run the following command to view recorded video at different training steps.\n",
    "\n",
    "xvfb-run -s \"-screen 0 640x480x24\" ./01_cartpole.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frozen-lake Value-Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"FrozenLake8x8-v0\" #\"FrozenLake-v0\"\n",
    "GAMMA = 0.9\n",
    "TEST_EPISODES = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(ENV_NAME)\n",
    "        self.state = self.env.reset()\n",
    "        \n",
    "        self.rewards = collections.defaultdict(float)\n",
    "        self.transits = collections.defaultdict(collections.Counter)\n",
    "        self.values = collections.defaultdict(float)\n",
    "\n",
    "    # introducing enough exploration providing stochasticity        \n",
    "    def play_n_random_steps(self, count):\n",
    "        '''\n",
    "        ues selection action find the best action to take and plays one \n",
    "        full episode using the provided environment, use to play test episodes without\n",
    "        effecting the current state of the main evironment used to gather random data.\n",
    "        '''\n",
    "        for _ in range(count):\n",
    "            action = self.env.action_space.sample()\n",
    "            \n",
    "            new_state, reward, is_done, _ = self.env.step(action)\n",
    "            \n",
    "            self.rewards[(self.state, action, new_state)] = reward\n",
    "            self.transits[(self.state, action)][new_state] += 1\n",
    "            \n",
    "            self.state = self.env.reset() if is_done else new_state\n",
    "            \n",
    "    # Calculate/Approximate value for each state and action using Bellman equation\n",
    "    def calc_action_value(self, state, action): # not used in Q-learning (or Q-iteration??) approach replaced by value table\n",
    "        target_counts = self.transits[(state, action)]\n",
    "        total = sum(target_counts.values())\n",
    "        action_value = 0.0\n",
    "        \n",
    "        for tgt_state, count in target_counts.items():\n",
    "            reward = self.rewards[(state, action, tgt_state)]\n",
    "            action_value += (count / total) * (reward + GAMMA * self.values[tgt_state])\n",
    "        return action_value\n",
    "    \n",
    "    # Use to make decision about the best action to take from the given state\n",
    "    def select_action(self, state): \n",
    "        best_action, best_value = None, None\n",
    "        # iterate over all possible actions in the environment and calculates value for each action\n",
    "        for action in range(self.env.action_space.n):\n",
    "            action_value = self.calc_action_value(state, action) # Q-learning (or Q-iteration??) uses self.values[(state, action)]\n",
    "            # The action with the largest value wins and is returned as the action to take.\n",
    "            if best_value is None or best_value < action_value:\n",
    "                # agent will behave greedily in regard to the value approximiation\n",
    "                best_value = action_value\n",
    "                best_action = action\n",
    "        return best_action\n",
    "\n",
    "    def play_episode(self, env):\n",
    "        total_reward = 0.0\n",
    "        state = env.reset()\n",
    "        \n",
    "        while True:\n",
    "            action = self.select_action(state)\n",
    "            \n",
    "            new_state, reward, is_done, _ = env.step(action)\n",
    "            \n",
    "            self.rewards[(state, action, new_state)] = reward\n",
    "            self.transits[(state, action)][new_state] += 1\n",
    "            \n",
    "            total_reward += reward\n",
    "            \n",
    "            if is_done:\n",
    "                break\n",
    "                \n",
    "            state = new_state\n",
    "            \n",
    "        return total_reward\n",
    "    \n",
    "    def value_iteration(self):\n",
    "        '''\n",
    "        loop over all states in the evironment, then for every state we calculate the values\n",
    "        for states reachable from it, obtaining candidates for the value of the state. \n",
    "        then update the value of the current state with the maximum value of the action \n",
    "        avaiable from the state.\n",
    "        '''\n",
    "        for state in range(self.env.observation_space.n):\n",
    "            state_values = [self.calc_action_value(state, action)\n",
    "                            for action in range(self.env.action_space.n)]\n",
    "            \n",
    "            self.values[state] = max(state_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = gym.make(ENV_NAME)\n",
    "agent = Agent()\n",
    "writer = SummaryWriter(comment=\"-v-iteration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best reward updated 0.000 -> 0.050\n",
      "Best reward updated 0.050 -> 0.150\n",
      "Best reward updated 0.150 -> 0.250\n",
      "Best reward updated 0.250 -> 0.300\n",
      "Best reward updated 0.300 -> 0.350\n",
      "Best reward updated 0.350 -> 0.400\n",
      "Best reward updated 0.400 -> 0.500\n",
      "Best reward updated 0.500 -> 0.550\n",
      "Best reward updated 0.550 -> 0.650\n",
      "Best reward updated 0.650 -> 0.700\n",
      "Best reward updated 0.700 -> 0.750\n",
      "Best reward updated 0.750 -> 0.800\n",
      "Best reward updated 0.800 -> 0.850\n",
      "Solved in 935 iterations!\n"
     ]
    }
   ],
   "source": [
    "iter_no = 0\n",
    "best_reward = 0.0\n",
    "\n",
    "while True:\n",
    "    iter_no += 1\n",
    "    # perform 100 random steps to fill out the reward and transition tables with fresh data\n",
    "    agent.play_n_random_steps(100)\n",
    "    # run value iteration over all states\n",
    "    agent.value_iteration()\n",
    "    \n",
    "    reward = 0.0\n",
    "    # use test episodes using the value table as the policy\n",
    "    for _ in range(TEST_EPISODES):\n",
    "        reward += agent.play_episode(test_env)\n",
    "        \n",
    "    reward /= TEST_EPISODES\n",
    "    # track the best average reward\n",
    "    writer.add_scalar(\"reward\", reward, iter_no)\n",
    "    \n",
    "    if reward > best_reward:\n",
    "        print(\"Best reward updated %.3f -> %.3f\" % (best_reward, reward))\n",
    "        best_reward = reward\n",
    "    # check for the training loop stop condition\n",
    "    if reward > 0.80:\n",
    "        print(\"Solved in %d iterations!\" % iter_no)\n",
    "        break\n",
    "        \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frozen-lake Q-Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(ENV_NAME)\n",
    "        self.state = self.env.reset()\n",
    "        self.rewards = collections.defaultdict(float)\n",
    "        self.transits = collections.defaultdict(collections.Counter)\n",
    "        self.values = collections.defaultdict(float)\n",
    "\n",
    "    def play_n_random_steps(self, count):\n",
    "        for _ in range(count):\n",
    "            action = self.env.action_space.sample()\n",
    "            new_state, reward, is_done, _ = self.env.step(action)\n",
    "            self.rewards[(self.state, action, new_state)] = reward\n",
    "            self.transits[(self.state, action)][new_state] += 1\n",
    "            self.state = self.env.reset() if is_done else new_state\n",
    "\n",
    "    def select_action(self, state):\n",
    "        best_action, best_value = None, None\n",
    "        for action in range(self.env.action_space.n):\n",
    "            action_value = self.values[(state, action)]\n",
    "            if best_value is None or best_value < action_value:\n",
    "                best_value = action_value\n",
    "                best_action = action\n",
    "        return best_action\n",
    "\n",
    "    def play_episode(self, env):\n",
    "        total_reward = 0.0\n",
    "        state = env.reset()\n",
    "        \n",
    "        while True:\n",
    "            action = self.select_action(state)\n",
    "            new_state, reward, is_done, _ = env.step(action)\n",
    "            self.rewards[(state, action, new_state)] = reward\n",
    "            self.transits[(state, action)][new_state] += 1\n",
    "            total_reward += reward\n",
    "            if is_done:\n",
    "                break\n",
    "            state = new_state\n",
    "        return total_reward\n",
    "    # Q-learning (or Q-iteration??)\n",
    "    def value_iteration(self):\n",
    "        for state in range(self.env.observation_space.n):\n",
    "            for action in range(self.env.action_space.n):\n",
    "                action_value = 0.0\n",
    "                target_counts = self.transits[(state, action)]\n",
    "                total = sum(target_counts.values())\n",
    "                for tgt_state, count in target_counts.items():\n",
    "                    reward = self.rewards[(state, action, tgt_state)]\n",
    "                    best_action = self.select_action(tgt_state)\n",
    "                    # Bellman equation\n",
    "                    action_value += (count / total) * (reward + GAMMA * self.values[(tgt_state, best_action)])\n",
    "                self.values[(state, action)] = action_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"FrozenLake-v0\"\n",
    "GAMMA = 0.9\n",
    "TEST_EPISODES = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = gym.make(ENV_NAME)\n",
    "agent = Agent()\n",
    "writer = SummaryWriter(comment=\"-q-iteration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best reward updated 0.000 -> 0.200\n",
      "Best reward updated 0.200 -> 0.300\n",
      "Best reward updated 0.300 -> 0.450\n",
      "Best reward updated 0.450 -> 0.500\n",
      "Best reward updated 0.500 -> 0.650\n",
      "Best reward updated 0.650 -> 0.750\n",
      "Best reward updated 0.750 -> 0.800\n",
      "Best reward updated 0.800 -> 0.850\n",
      "Solved in 61 iterations!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method SummaryWriter.close of <tensorboardX.writer.SummaryWriter object at 0x113317320>>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter_no = 0\n",
    "best_reward = 0.0\n",
    "\n",
    "while True:\n",
    "    iter_no += 1\n",
    "    agent.play_n_random_steps(100)\n",
    "    agent.value_iteration()\n",
    "    \n",
    "    reward = 0.0\n",
    "    \n",
    "    for _ in range(TEST_EPISODES):\n",
    "        reward += agent.play_episode(test_env)\n",
    "        \n",
    "    reward /= TEST_EPISODES\n",
    "    \n",
    "    writer.add_scalar(\"reward\", reward, iter_no)\n",
    "    \n",
    "    if reward > best_reward:\n",
    "        print(\"Best reward updated %.3f -> %.3f\" % (best_reward, reward))\n",
    "        best_reward = reward\n",
    "    if reward > 0.80:\n",
    "        print(\"Solved in %d iterations!\" % iter_no)\n",
    "        break\n",
    "\n",
    "writer.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frozen-lake Tabular Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(ENV_NAME)\n",
    "        self.state = self.env.reset()\n",
    "        self.values = collections.defaultdict(float)\n",
    "    \n",
    "    def sample_env(self):\n",
    "        \"\"\"\n",
    "        Do not have to track the history of rewards and transition counters, just the value table\n",
    "        \"\"\"\n",
    "        action = self.env.action_space.sample()\n",
    "        old_state = self.state\n",
    "        # take action\n",
    "        new_state, reward, is_done, _ = self.env.step(action)\n",
    "        \n",
    "        return (old_state, action, reward, new_state)\n",
    "    \n",
    "    def best_value_and_action(self, state):\n",
    "        \"\"\"\n",
    "        Receives the state of the environment and finds the best action to take from this state by taking\n",
    "        the action with the largest value that will be in the table. \n",
    "        \n",
    "        This method will be used two times: \n",
    "        1. to evaluate the policy quality, in the test method that plays one episode using the current values table.\n",
    "        2. in the method that performs the value update to get the value of the next state.\n",
    "        \"\"\"\n",
    "        best_value, best_action = None, None\n",
    "        for action in range(self.env.action_space.n):\n",
    "            action_value = self.values[(state, action)]\n",
    "            if best_value is None or best_value < action_value:\n",
    "                best_value = action_value\n",
    "                best_action = action\n",
    "        return best_value, best_action\n",
    "    \n",
    "    def value_update(self, s, a, r, next_s):\n",
    "        \"\"\"\n",
    "        We update the values table using one-step from the environment by using calculating the Bellman approximaiton\n",
    "        for the state s and action a by summing over immediate reward with the discounted value of the next state.\n",
    "        Method also used to perform the value update to get the value of the next state.\n",
    "        \n",
    "        \"\"\"\n",
    "        best_v, _ = self.best_value_and_action(next_s)\n",
    "        new_val = r + GAMMA * best_v\n",
    "        old_val = self.values[(s, a)]\n",
    "        # obtaining the previous value of the state and action pair used to blend these values together using\n",
    "        # the learning rate with the older value.\n",
    "        self.values[(s, a)] = (1 - ALPHA) * old_val + ALPHA * new_val \n",
    "        \n",
    "    def play_episode(self, env):\n",
    "        \"\"\"\n",
    "        Agent plays one full episode using the current values table (provided test environment).\n",
    "        This method is used to evaluate the current policy to check the progress of learning.\n",
    "        Note this method does not alter the value table it only uses it to find the best action to take.\n",
    "        \"\"\"\n",
    "        total_reward = 0.0\n",
    "        state = env.reset()\n",
    "        \n",
    "        while True:\n",
    "            _, action = self.best_value_and_action(state)\n",
    "            # the action on every step is taken using the current value table of Q-values.\n",
    "            new_state, reward, is_done, _ = env.step(action)\n",
    "            \n",
    "            total_reward += reward\n",
    "            \n",
    "            if is_done:\n",
    "                break\n",
    "            state = new_state\n",
    "            \n",
    "        return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"FrozenLake-v0\"\n",
    "GAMMA = 0.9\n",
    "ALPHA = 0.2\n",
    "TEST_EPISODES = 1#20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = gym.make(ENV_NAME)\n",
    "agent = Agent()\n",
    "writer = SummaryWriter(comment=\"-q-learning\") # Tabular Q-learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a test environment, agent, and summary writer, then in the loop, we do one-step in the environment and perform a value update using the obtained data. Then we test the current policy by playing several test episodes. If a good reward is obtained, then we stop training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-44a31ddd594f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# we test the current policy by playing several test episodes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEST_EPISODES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_env\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mreward\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mTEST_EPISODES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-abafe660c378>\u001b[0m in \u001b[0;36mplay_episode\u001b[0;34m(self, env)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mplay_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.7/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.7/site-packages/gym/envs/toy_text/discrete.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcategorical_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlastaction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.7/site-packages/gym/envs/toy_text/discrete.py\u001b[0m in \u001b[0;36mcategorical_sample\u001b[0;34m(prob_n, np_random)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprob_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mcsprob_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcsprob_n\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mnp_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "iter_no = 0\n",
    "best_reward = 0.0\n",
    "\n",
    "# training loop\n",
    "while True:\n",
    "    # do one step in the environment and perform update using the obtained data.\n",
    "    iter_no += 1\n",
    "    s, a, r, next_s = agent.sample_env()\n",
    "    agent.value_update(s, a, r, next_s)\n",
    "    \n",
    "    reward = 0.0\n",
    "    \n",
    "    # we test the current policy by playing several test episodes. \n",
    "    for _ in range(TEST_EPISODES):\n",
    "        reward += agent.play_episode(test_env)\n",
    "\n",
    "    reward /= TEST_EPISODES\n",
    "    \n",
    "    writer.add_scalar(\"reward\", reward, iter_no)\n",
    "    \n",
    "    # test result of current policy\n",
    "    if reward > best_reward:\n",
    "        print(\"Best reward updated %.3f -> %.3f\" % (best_reward, reward))\n",
    "        best_reward = reward\n",
    "    \n",
    "    # If a good reward is obtained then stop training.\n",
    "    if reward > 0.80:\n",
    "        print(\"Solved in %d iterations!\" % iter_no)\n",
    "        break\n",
    "        \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frozen Lake with Q-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym,sys,numpy as np\n",
    "import tensorflow as tf\n",
    "from gym.envs.registration import register\n",
    "\n",
    "np.random.seed(56776)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "register(\n",
    "    id='FrozenLakeNotSlippery-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
    "    max_episode_steps=2000,\n",
    "    reward_threshold=0.78, # optimum = .8196\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the env\n",
    "env = gym.make('FrozenLakeNotSlippery-v0')\n",
    "env.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_learning_table = np.zeros([env.observation_space.n,env.action_space.n])\n",
    "print(q_learning_table)\n",
    "print(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- hyper --\n",
    "num_epis = 5000\n",
    "num_iter = 2000\n",
    "learning_rate = 0.3\n",
    "discount = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- training the agent ----\n",
    "for epis in range(num_epis):\n",
    "    state = env.reset()\n",
    "    for iter in range(num_iter):\n",
    "        action = np.argmax(q_learning_table[state,:] + np.random.randn(1,4))\n",
    "        state_new,reward,done,_ = env.step(action)\n",
    "        q_learning_table[state,action] = (1-learning_rate)* q_learning_table[state,action] + \\\n",
    "                                         learning_rate * (reward + discount*np.max(q_learning_table[state_new,:]) )\n",
    "        state = state_new\n",
    "        if done: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.argmax(q_learning_table,axis=1))\n",
    "print(np.around(q_learning_table,6))\n",
    "print('-------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize no uncertainty\n",
    "s = env.reset()\n",
    "for _ in range(100):\n",
    "    action  = np.argmax(q_learning_table[s,:])\n",
    "    state_new,_,done,_ = env.step(action)\n",
    "    env.render()\n",
    "    s = state_new\n",
    "    if done: break\n",
    "print('-------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the env\n",
    "env = gym.make('FrozenLake-v0')\n",
    "env.seed(0)\n",
    "np.random.seed(56776)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_learning_table = np.zeros([env.observation_space.n,env.action_space.n])\n",
    "print(q_learning_table)\n",
    "print(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- hyper --\n",
    "num_epis = 500\n",
    "num_iter = 200\n",
    "learning_rate = 0.3\n",
    "discount = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- training the agent ----\n",
    "for epis in range(num_epis):\n",
    "    \n",
    "    state = env.reset()\n",
    "\n",
    "    for iter in range(num_iter):\n",
    "        action = np.argmax(q_learning_table[state,:] + np.random.randn(1,4))\n",
    "        state_new,reward,done,_ = env.step(action)\n",
    "        q_learning_table[state,action] = (1-learning_rate)* q_learning_table[state,action] + \\\n",
    "                                         learning_rate * (reward + discount*np.max(q_learning_table[state_new,:]) )\n",
    "        state = state_new\n",
    "\n",
    "        if done: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.argmax(q_learning_table,axis=1))\n",
    "print(np.around(q_learning_table,6))\n",
    "print('-------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = env.reset()\n",
    "for _ in range(100):\n",
    "    action  = np.argmax(q_learning_table[s,:])\n",
    "    state_new,_,done,_ = env.step(action)\n",
    "    env.render()\n",
    "    s = state_new\n",
    "    if done: break\n",
    "# -- end code --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning: Q-learning with value iteration\n",
    "\n",
    "### Frozen Lake Game\n",
    "\n",
    "winter is here. You and your friends where tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is splippery, so you won't always move in the diretion you intended. Teh surface is described by using a grid like the following:\n",
    "    \n",
    "    SFFF\n",
    "    FHFH\n",
    "    FFFH\n",
    "    HFFG\n",
    "    \n",
    "   Where S stands for start where the agent begins, F for frozen surface (safe), H for hole, and G for goal of getting frisbee. \n",
    "    \n",
    "\n",
    "\n",
    "### Setting library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can query information about the environment, sample states and actions, \n",
    "# retreive rewards, and have agent navigate the envorinment\n",
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create the Q-table and initialize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of the action space \n",
    "action_space_size = env.action_space.n\n",
    "# size of the state space in the environment\n",
    "state_space_size = env.observation_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build Q-table and fill it with zeros\n",
    "q_table = np.zeros((state_space_size, action_space_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Initializing Q-learning hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and initialize all the parameters needed to implement the Q-learning algorithm.\n",
    "num_episodes = 10000 # number of plays during training\n",
    "max_steps_per_episode = 100 # max number of steps agent allowed to take in a single episode\n",
    "\n",
    "learning_rate = 0.1\n",
    "discount_rate = 0.99\n",
    "\n",
    "# related to the exploration-exploitation trade-off using epsilon-greedy policy\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01   # max and min are the bounds to how large or small our exploration rate can be\n",
    "exploration_decay_rate = 0.001\n",
    "\n",
    "# Note: exploration rate was represented by epsilon "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The lower the exploration decay rate, the longer the agent will be able to explore. With 0.01 as the decay rate, the agent was only able to explore for a relatively short amount of time until it went into full exploitation mode without having a chance to fully explore and learn about the environment. Decreasing the decay rate to 0.001 allowed the agent to explore for longer and learn more.\n",
    "\n",
    "If the exploration decay rate is too large, the 2nd term in the exploration-rate update is  0 (because the exponential term is  0). The impact is that subsequent epsilon-greedy searches get stuck in an exploitation mode, since the exploration rate converges to \"min_exploration_rate\" (little to no exploration occurs). \n",
    "\n",
    "These behavior would come much more clear if the game was deterministic (no slipping on ice), since the slippery situation adds a randomness which contributes to hide the phenomenon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: The Q learning algorithm\n",
    "\n",
    "Source:\n",
    "    - https://www.kaggle.com/sandovaledwin/q-learning-algorithm-for-solving-frozenlake-game/code\n",
    "    - http://deeplizard.com/learn/video/HGeI30uATws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list to hold all of the rewards well get from each episode, to see how our game score changes over time.\n",
    "rewards_all_episodes = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Q-learning algorithm\n",
    "\n",
    "This first for-loop contains everything that happens within a single episode. This second nested loop contains everything that happens for a single time-step.\n",
    "\n",
    "\n",
    "##### Update Q-value formula:\n",
    "\n",
    "$$q(s,a)\\;=\\;(1 - \\alpha) + \\alpha(R_{t+1} + \\gamma*max_{a'}\\;q(s',a'))$$\n",
    "\n",
    "Used to update the Q-table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Q-learing algorithm \n",
    "\n",
    "# everything that happends within a single episode\n",
    "for episode in range(num_episodes):\n",
    "    # reset the state of the environment back to the starting state\n",
    "    state = env.reset() \n",
    "    \n",
    "    # initialize new episode params\n",
    "    done = False # keeps track of whether or not the episode is finished so initialize to false for the starting\n",
    "    \n",
    "    # keep track of the rewards within the current episode \n",
    "    rewards_current_episode = 0 # set to zero since we start out with no rewards at the beginning of each episode\n",
    "    \n",
    "    # everything that happends within a single time-step within each episode\n",
    "    for step in range(max_steps_per_episode):\n",
    "        \n",
    "        # Exploration-exploitation trade-off\n",
    "        exploration_rate_threshold = random.uniform(0, 1) # epsilon: probability of exploring or exploiting the environment on this timne step\n",
    "        \n",
    "        #if exploration_rate_threshold > exploration_rate and ~np.all(q_table[state,:]==0):\n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            # Exploitation: Take new action with Greedy Policy, only if the q values for the state are NOT all 0\n",
    "            action = np.argmax(q_table[state,:])\n",
    "        else:\n",
    "            # Explore: Take new action\n",
    "            action = env.action_space.sample()\n",
    "            #print('Exploration')\n",
    "        \n",
    "        # Returns a tuple containing the new state. And 'info' diagnostic information regarding our environment\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Update Q-table for Q(s,a) is a weighted sum of our old value and the learned value.\n",
    "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) \\\n",
    "        + learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
    "        \n",
    "        # Transition to the next state\n",
    "        # Set new state\n",
    "        state = new_state\n",
    "        \n",
    "        # Add new reward \n",
    "        rewards_current_episode += reward\n",
    "        \n",
    "        # Check to see if our last action ended the episode (did our agent step in a hole or reach the goal?)\n",
    "        if done == True:\n",
    "            break\n",
    "            \n",
    "    # Exploration rate decay\n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "    (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate * episode)\n",
    "     \n",
    "    # Add current episode reward to total rewards list\n",
    "    rewards_all_episodes.append(rewards_current_episode)                                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After all episodes complete\n",
    "\n",
    "we now just calculate the average reward per thousand episodes from our list that contains the rewards for all episodes so that we can print it out and see how the rewards changed over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print the average reward per thousand episodes\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes), num_episodes/1000)\n",
    "count = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** Average reward per thousand episodes ********\n",
      "\n",
      "1000  0.04200000000000003\n",
      "2000  0.22700000000000017\n",
      "3000  0.3880000000000003\n",
      "4000  0.5270000000000004\n",
      "5000  0.6160000000000004\n",
      "6000  0.6430000000000005\n",
      "7000  0.6970000000000005\n",
      "8000  0.6910000000000005\n",
      "9000  0.6970000000000005\n",
      "10000  0.7000000000000005\n"
     ]
    }
   ],
   "source": [
    "print(\"******** Average reward per thousand episodes ********\\n\")\n",
    "for reward in rewards_per_thousand_episodes:\n",
    "    print(count, \"\", str(sum(reward/1000)))\n",
    "    count += 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "******** Q-table ********\n",
      "\n",
      "[[0.55305969 0.50936222 0.51268992 0.51292919]\n",
      " [0.36488135 0.33360739 0.26946678 0.50493763]\n",
      " [0.44790108 0.44540804 0.44219952 0.47713909]\n",
      " [0.22650871 0.21555082 0.29339466 0.4616523 ]\n",
      " [0.57257193 0.34338435 0.42032957 0.42608411]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.41637697 0.19327921 0.18684407 0.13209898]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.36310989 0.45718635 0.29683875 0.60511954]\n",
      " [0.40807201 0.7130411  0.43457842 0.40485577]\n",
      " [0.68275394 0.4480717  0.42565895 0.33215237]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.40610197 0.30692862 0.80346477 0.39405881]\n",
      " [0.76452398 0.89945059 0.80661165 0.79968618]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Print updated Q-table to see how that has transitioned from its initial state of all zeros.\n",
    "print(\"\\n\\n******** Q-table ********\\n\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent played 10,000 episodes, within each time step within an episode the agent received a reward of one if it reached the frisbee and otherwise it received a reward of zero. If the agen did indeed reach the frisbee then the episode finished at that time step. This means that for each episode the total reward received by the agent for the entire episode is either one or zero. For the first thousand episodes we can interpret the first score in the printout as meaning that 5% of the time the agent recieved a reward of one and won the episode. And by the last thousand episodes from a total of 10,000 the agent was winning 70% of the time and from the grid enviroment we can see that it is more likely for the agent to fall in a hole or perhaps reach the  the max number of time steps than it is to reach the frisbee. But reach the frisbee 70% of the time by the end of the training is not too bad especially since the agent had not explicit instructions to reach the frisbee in any case. It learned through reinforcement that this is the correct direction/action to do.\n",
    "\n",
    "We observe from the print out that over time during training we can see the average rewards per thousand episodes did indeed progress over time. When the algorithm first started training for the first thousand episodes only averaged a rewards of 0.005 but the time it got to its last thousand episodes the reward drastically improved to 0.694. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of steps 99\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Use our Q-table to play FrozenLake !\n",
    "\n",
    "# List of rewards\n",
    "rewards = []\n",
    "\n",
    "# Watch our agent play Frozen Lake by playing the best action \n",
    "# from each state according to the Q-table\n",
    "for episode in range(3): # watch the agent play three episodes\n",
    "    # Reset the environment\n",
    "    state = env.reset()\n",
    "    \n",
    "    # initialize new episode params\n",
    "    step = 0\n",
    "    done = False\n",
    "    print('****** EPISODE ', episode+1, '*******\\n\\n\\n')\n",
    "    time.sleep(1) # allow print out to be read before disappearing\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        # clear output from jupyter notebook cell\n",
    "        clear_output(wait=True) \n",
    "        \n",
    "        # Show current state of environment on screen\n",
    "        env.render() # render current state of the environment to display where the agent is in the grid (visually see the game grid)\n",
    "        time.sleep(0.3) # sleep 300 milliseconds, will allow to see the current state of the environment before moving on to the next time step\n",
    "        \n",
    "        # Choose action with highest Q-value for current state\n",
    "        # Take the action (index) that have the maximum expected future reward given that state.\n",
    "        # Set the action to the action with the highest Q-value in the Q-table for the current sate.\n",
    "        action = np.argmax(q_table[state,:])\n",
    "        \n",
    "        # Take the new action (a) and observe the outcome state(s') and reward (r)\n",
    "        new_state, reward, done, info = env.step(action) # take action\n",
    "        \n",
    "        # If done (if we're dead) : finish episode\n",
    "        if done:\n",
    "            # Here, we decide to only print the last state (to see if our agent is on the goal or fall into an hole)\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward == 1: # Agent reached the goal and won episode\n",
    "                print('***** You reached the goal! *****')\n",
    "                time.sleep(3)\n",
    "            else: # Agent stepped in a hole and lost episode\n",
    "                print('**** You fell through a hole! *****')\n",
    "                time.sleep(3)\n",
    "                \n",
    "            # clear output from jupyter cell\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            # We print the number of step it took.\n",
    "            print(\"Number of steps\", step)\n",
    "            break\n",
    "            \n",
    "        # Set our new state\n",
    "        state = new_state\n",
    "\n",
    "# After all three episodes are done, we then close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
